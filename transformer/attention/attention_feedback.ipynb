{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "without PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense, MultiHeadAttention\n",
      "File \u001b[0;32m~/Documents/ok_environment/Mensius/lib/python3.12/site-packages/tensorflow/__init__.py:45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/Documents/ok_environment/Mensius/lib/python3.12/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[0;32m~/Documents/ok_environment/Mensius/lib/python3.12/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ok_environment/Mensius/lib/python3.12/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[0;32m~/Documents/ok_environment/Mensius/lib/python3.12/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[0;32m~/Documents/ok_environment/Mensius/lib/python3.12/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[0;32m~/Documents/ok_environment/Mensius/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:45\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# pywrap_tensorflow must be imported first to avoid protobuf issues.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# (b/143110113)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# pylint: enable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ok_environment/Mensius/lib/python3.12/site-packages/tensorflow/python/pywrap_tensorflow.py:34\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m self_check\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mself_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreload_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m   \u001b[38;5;66;03m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;66;03m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ok_environment/Mensius/lib/python3.12/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the DLL(s) \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. TensorFlow requires that these DLLs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe installed in a directory that is named in your \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;66;03m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;66;03m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[38;5;241m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention\n",
    "# Create a figure to visualize the training history\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ Define Fixed Word Embeddings (Predefined & Not Trainable)\n",
    "eng_vocab = {\"I\": 0, \"love\": 1, \"cats\": 2}\n",
    "chn_vocab = {\"我\": 0, \"喜\": 1, \"歡\": 2, \"貓\": 3}  # \"喜\" and \"歡\" are separate\n",
    "\n",
    "eng_vocab_size = len(eng_vocab)  \n",
    "chn_vocab_size = len(chn_vocab)  \n",
    "seq_len = 3  # English sequence length\n",
    "target_seq_len = 4  # Chinese sequence length (now 4 words)\n",
    "embed_dim = 8  # Word vector size\n",
    "num_heads = 1  # Single-head attention\n",
    "\n",
    "# **Fixed Word Embeddings (Manually Set)**\n",
    "eng_embeddings = np.array([\n",
    "    [0.1, 0.3, -0.5, 0.7, 0.2, -0.4, 0.6, -0.1],  # \"I\"\n",
    "    [0.2, -0.1, 0.6, -0.3, 0.5, 0.4, -0.2, 0.1],  # \"love\"\n",
    "    [-0.4, 0.8, 0.2, -0.6, 0.3, -0.7, 0.9, 0.0]   # \"cats\"\n",
    "])\n",
    "\n",
    "chn_embeddings = np.array([\n",
    "    [-0.2, 0.5, 0.1, -0.3, 0.7, 0.6, -0.8, 0.2],  # \"我\"\n",
    "    [0.3, -0.7, 0.8, 0.2, -0.6, 0.1, 0.5, -0.4],  # \"喜\"\n",
    "    [0.1, 0.6, -0.2, -0.5, 0.8, -0.3, 0.4, -0.7], # \"歡\"\n",
    "    [0.5, 0.4, -0.1, -0.6, 0.2, -0.9, 0.7, 0.3]   # \"貓\"\n",
    "])\n",
    "\n",
    "# 2️⃣ Define Encoder (Self-Attention)\n",
    "encoder_inputs = Input(shape=(seq_len, embed_dim))  # Fixed input embeddings\n",
    "encoder_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "encoder_output = encoder_attention(encoder_inputs, encoder_inputs)  # Self-attention in encoder\n",
    "\n",
    "# 3️⃣ Define Decoder with Feedback\n",
    "decoder_inputs = Input(shape=(target_seq_len, embed_dim))  # Decoder input\n",
    "decoder_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "decoder_output = decoder_attention(decoder_inputs, encoder_output)  # Decoder attends to encoder\n",
    "\n",
    "# 4️⃣ Fully Connected Output Layer (Predicts Chinese Words)\n",
    "dense_layer = Dense(chn_vocab_size, activation=\"softmax\")  # Outputs probabilities for each word\n",
    "outputs = dense_layer(decoder_output)\n",
    "\n",
    "# 5️⃣ Compile the Model\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 6️⃣ Prepare Training Data (Using Fixed Embeddings)\n",
    "X_train_eng = np.array([eng_embeddings])  # Fixed embeddings for \"I love cats\"\n",
    "X_train_chn = np.zeros((1, target_seq_len, embed_dim))  # Empty decoder input at start\n",
    "\n",
    "y_train = np.array([\n",
    "    [0, 1, 2, 3]  # \"我 喜 歡 貓\" (Target sequence)\n",
    "])\n",
    "\n",
    "# 7️⃣ Train the Model\n",
    "model.fit([X_train_eng, X_train_chn], y_train, epochs=1000, batch_size=1)\n",
    "\n",
    "# 8️⃣ Test Translation (Autoregressive Generation)\n",
    "def translate_sentence():\n",
    "    encoder_input = np.array([eng_embeddings])  # Fixed English embeddings\n",
    "    decoder_input = np.zeros((1, target_seq_len, embed_dim))  # Start with empty decoder input\n",
    "\n",
    "    translated_sentence = []\n",
    "    \n",
    "    for i in range(target_seq_len):  # Generate word-by-word\n",
    "        pred_probs = model.predict([encoder_input, decoder_input])[0, i, :]\n",
    "        predicted_word_index = np.argmax(pred_probs)  # Select highest probability word\n",
    "        translated_word = list(chn_vocab.keys())[list(chn_vocab.values()).index(predicted_word_index)]\n",
    "        \n",
    "        translated_sentence.append(translated_word)\n",
    "        \n",
    "        decoder_input[0, i, :] = chn_embeddings[predicted_word_index]  # Feed output back into decoder\n",
    "\n",
    "    return \" \".join(translated_sentence)\n",
    "\n",
    "# Generate Translation\n",
    "generated_translation = translate_sentence()\n",
    "print(\"Predicted Translation:\", generated_translation)  # Expected: \"我 喜 歡 貓\"\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss During Training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with one-hot PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.2500 - loss: 1.3925\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3915\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3906\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2500 - loss: 1.3898\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2500 - loss: 1.3891\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2500 - loss: 1.3885\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2500 - loss: 1.3880\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2500 - loss: 1.3876\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3873\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3870\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3868\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3867\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3865\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3865\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3865\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3865\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(translated_sentence)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Generate Translation\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m generated_translation \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_sentence\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Translation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, generated_translation)  \u001b[38;5;66;03m# Expected: \"我 喜 歡 貓\"\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[25], line 95\u001b[0m, in \u001b[0;36mtranslate_sentence\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m     translated_sentence\u001b[38;5;241m.\u001b[39mappend(translated_word)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Add positional encoding dynamically\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     decoder_input[\u001b[38;5;241m0\u001b[39m, i, :] \u001b[38;5;241m=\u001b[39m chn_embeddings[predicted_word_index] \u001b[38;5;241m+\u001b[39m \u001b[43mpos_encoding\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Apply PE at correct step\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(translated_sentence)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 6"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ Define Fixed Word Embeddings (Predefined & Not Trainable)\n",
    "eng_vocab = {\"I\": 0, \"love\": 1, \"cats\": 2}\n",
    "chn_vocab = {\"我\": 0, \"喜\": 1, \"歡\": 2, \"貓\": 3}  # \"喜\" and \"歡\" are separate\n",
    "\n",
    "eng_vocab_size = len(eng_vocab)  \n",
    "chn_vocab_size = len(chn_vocab)  \n",
    "seq_len = 3  # English sequence length\n",
    "target_seq_len = 4  # Chinese sequence length (now 4 words)\n",
    "embed_dim = 6  # Word vector size reduced to 6\n",
    "num_heads = 1  # Single-head attention\n",
    "\n",
    "# **Fixed Word Embeddings (Manually Set, 6D)**\n",
    "eng_embeddings = np.array([\n",
    "    [0.1, 0.3, -0.5, 0.7, 0.2, -0.4],  # \"I\"\n",
    "    [0.2, -0.1, 0.6, -0.3, 0.5, 0.4],  # \"love\"\n",
    "    [-0.4, 0.8, 0.2, -0.6, 0.3, -0.7]   # \"cats\"\n",
    "])\n",
    "\n",
    "chn_embeddings = np.array([\n",
    "    [-0.2, 0.5, 0.1, -0.3, 0.7, 0.6],  # \"我\"\n",
    "    [0.3, -0.7, 0.8, 0.2, -0.6, 0.1],  # \"喜\"\n",
    "    [0.1, 0.6, -0.2, -0.5, 0.8, -0.3], # \"歡\"\n",
    "    [0.5, 0.4, -0.1, -0.6, 0.2, -0.9]  # \"貓\"\n",
    "])\n",
    "\n",
    "# **Extended Fixed Positional Encoding (6D One-Hot)**\n",
    "pos_encoding = np.array([\n",
    "    [1, 0, 0, 0, 0, 0],  # Position 0\n",
    "    [0, 1, 0, 0, 0, 0],  # Position 1\n",
    "    [0, 0, 1, 0, 0, 0],  # Position 2\n",
    "    [0, 0, 0, 1, 0, 0],  # Position 3\n",
    "    [0, 0, 0, 0, 1, 0],  # Position 4\n",
    "    [0, 0, 0, 0, 0, 1]   # Position 5\n",
    "])\n",
    "\n",
    "# 2️⃣ Define Encoder (Self-Attention)\n",
    "encoder_inputs = Input(shape=(seq_len, embed_dim))  # Fixed input embeddings\n",
    "encoder_positional = Input(shape=(seq_len, embed_dim))  # Positional encoding\n",
    "encoder_combined = encoder_inputs + encoder_positional  # Add position info\n",
    "\n",
    "encoder_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "encoder_output = encoder_attention(encoder_combined, encoder_combined)  # Self-attention\n",
    "\n",
    "# 3️⃣ Define Decoder with Feedback\n",
    "decoder_inputs = Input(shape=(target_seq_len, embed_dim))  # Decoder input\n",
    "decoder_positional = Input(shape=(target_seq_len, embed_dim))  # Positional encoding\n",
    "decoder_combined = decoder_inputs + decoder_positional  # Add position info\n",
    "\n",
    "decoder_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "decoder_output = decoder_attention(decoder_combined, encoder_output)  # Decoder attends to encoder\n",
    "\n",
    "# 4️⃣ Fully Connected Output Layer (Predicts Chinese Words)\n",
    "dense_layer = Dense(chn_vocab_size, activation=\"softmax\")  # Outputs probabilities for each word\n",
    "outputs = dense_layer(decoder_output)\n",
    "\n",
    "# 5️⃣ Compile the Model\n",
    "model = keras.Model(inputs=[encoder_inputs, encoder_positional, decoder_inputs, decoder_positional], outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 6️⃣ Prepare Training Data (Using Fixed Embeddings)\n",
    "X_train_eng = np.array([eng_embeddings])  # Fixed embeddings for \"I love cats\"\n",
    "X_train_eng_pos = np.array([pos_encoding[:seq_len]])  # English positional encoding\n",
    "X_train_chn = np.zeros((1, target_seq_len, embed_dim)) + pos_encoding[:target_seq_len]  # Decoder input gets PE\n",
    "X_train_chn_pos = np.array([pos_encoding[:target_seq_len]])  # Chinese positional encoding\n",
    "\n",
    "y_train = np.array([\n",
    "    [0, 1, 2, 3]  # \"我 喜 歡 貓\" (Target sequence)\n",
    "])\n",
    "\n",
    "# 7️⃣ Train the Model\n",
    "model.fit([X_train_eng, X_train_eng_pos, X_train_chn, X_train_chn_pos], y_train, epochs=100, batch_size=1)\n",
    "\n",
    "# 8️⃣ Test Translation (Autoregressive Generation)\n",
    "def translate_sentence():\n",
    "    encoder_input = np.array([eng_embeddings])  # Fixed English embeddings\n",
    "    encoder_input_pos = np.array([pos_encoding[:seq_len]])  # Fixed positional encoding\n",
    "    decoder_input = np.zeros((1, target_seq_len, embed_dim)) + pos_encoding[:target_seq_len]  # Start with PE\n",
    "    decoder_input_pos = np.array([pos_encoding[:target_seq_len]])  # Fixed decoder positional encoding\n",
    "\n",
    "    translated_sentence = []\n",
    "    \n",
    "    for i in range(target_seq_len):  # Generate word-by-word\n",
    "        pred_probs = model.predict([encoder_input, encoder_input_pos, decoder_input, decoder_input_pos])[0, i, :]\n",
    "        predicted_word_index = np.argmax(pred_probs)  # Select highest probability word\n",
    "        translated_word = list(chn_vocab.keys())[list(chn_vocab.values()).index(predicted_word_index)]\n",
    "        \n",
    "        translated_sentence.append(translated_word)\n",
    "        \n",
    "        # Add positional encoding dynamically\n",
    "        decoder_input[0, i, :] = chn_embeddings[predicted_word_index] + pos_encoding[i + 3]  # Apply PE at correct step\n",
    "\n",
    "    return \" \".join(translated_sentence)\n",
    "\n",
    "# Generate Translation\n",
    "generated_translation = translate_sentence()\n",
    "print(\"Predicted Translation:\", generated_translation)  # Expected: \"我 喜 歡 貓\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 22:30:48.429354: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step - accuracy: 0.2500 - loss: 1.3868\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.2500 - loss: 1.3865\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 111ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 507ms/step - accuracy: 0.2500 - loss: 1.3864\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 206ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2500 - loss: 1.3863\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Predicted Translation: 貓 貓 貓 貓\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, MultiHeadAttention, Embedding\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ Define Transformer-Style Sinusoidal Positional Encoding\n",
    "def positional_encoding(seq_len, embed_dim):\n",
    "    PE = np.zeros((seq_len, embed_dim))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, embed_dim, 2):\n",
    "            denominator = np.power(10000, (2 * i) / embed_dim)\n",
    "            PE[pos, i] = np.sin(pos / denominator)  # Even indices: sine function\n",
    "            if i + 1 < embed_dim:\n",
    "                PE[pos, i + 1] = np.cos(pos / denominator)  # Odd indices: cosine function\n",
    "    return PE\n",
    "\n",
    "# 2️⃣ Define Vocabulary & Parameters\n",
    "eng_vocab = {\"I\": 0, \"love\": 1, \"cats\": 2}\n",
    "chn_vocab = {\"我\": 0, \"喜\": 1, \"歡\": 2, \"貓\": 3}\n",
    "\n",
    "eng_vocab_size = len(eng_vocab)  \n",
    "chn_vocab_size = len(chn_vocab)  \n",
    "seq_len = 3  # Input sentence length\n",
    "target_seq_len = 4  # Output sentence length\n",
    "embed_dim = 6  # Word embedding & PE dimension\n",
    "num_heads = 1  # Only one attention head\n",
    "\n",
    "# 3️⃣ Generate Transformer-Style Positional Encoding\n",
    "PE_eng = positional_encoding(seq_len, embed_dim)  # PE for English\n",
    "PE_chn = positional_encoding(target_seq_len, embed_dim)  # PE for Chinese\n",
    "\n",
    "# 4️⃣ Define Trainable Word Embeddings\n",
    "embedding_layer_eng = Embedding(input_dim=eng_vocab_size, output_dim=embed_dim, trainable=True)\n",
    "embedding_layer_chn = Embedding(input_dim=chn_vocab_size, output_dim=embed_dim, trainable=True)\n",
    "\n",
    "# 5️⃣ Define Encoder (Self-Attention)\n",
    "encoder_inputs = Input(shape=(seq_len,))  # Word indices\n",
    "embedded_inputs = embedding_layer_eng(encoder_inputs)  # Convert indices to embeddings\n",
    "encoder_positional = Input(shape=(seq_len, embed_dim))  # Positional Encoding\n",
    "encoder_combined = embedded_inputs + encoder_positional  # Add PE to embeddings\n",
    "\n",
    "encoder_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "encoder_output = encoder_attention(encoder_combined, encoder_combined)  # Self-attention\n",
    "\n",
    "# 6️⃣ Define Decoder\n",
    "decoder_inputs = Input(shape=(target_seq_len,))  # Target word indices\n",
    "embedded_decoder_inputs = embedding_layer_chn(decoder_inputs)  # Convert to embeddings\n",
    "decoder_positional = Input(shape=(target_seq_len, embed_dim))  # Positional Encoding\n",
    "decoder_combined = embedded_decoder_inputs + decoder_positional  # Add PE to embeddings\n",
    "\n",
    "decoder_attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "decoder_output = decoder_attention(decoder_combined, encoder_output)  # Attend to encoder\n",
    "\n",
    "# 7️⃣ Fully Connected Output Layer (Predicts Next Words)\n",
    "dense_layer = Dense(chn_vocab_size, activation=\"softmax\")  # Outputs probabilities\n",
    "outputs = dense_layer(decoder_output)\n",
    "\n",
    "# 8️⃣ Compile the Model\n",
    "model = keras.Model(inputs=[encoder_inputs, encoder_positional, decoder_inputs, decoder_positional], outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# 9️⃣ Prepare Training Data (Single Example)\n",
    "X_train_eng = np.array([[eng_vocab[\"I\"], eng_vocab[\"love\"], eng_vocab[\"cats\"]]])  # \"I love cats\"\n",
    "X_train_chn = np.array([[chn_vocab[\"我\"], chn_vocab[\"喜\"], chn_vocab[\"歡\"], chn_vocab[\"貓\"]]])  # \"我 喜 歡 貓\"\n",
    "\n",
    "X_train_eng_pos = np.tile(PE_eng, (1, 1, 1))  # Positional Encoding for English\n",
    "X_train_chn_pos = np.tile(PE_chn, (1, 1, 1))  # Positional Encoding for Chinese\n",
    "\n",
    "# 🔟 Train the Model\n",
    "model.fit([X_train_eng, X_train_eng_pos, X_train_chn, X_train_chn_pos], X_train_chn, epochs=100, batch_size=1)\n",
    "\n",
    "# 🔟 Test Translation (Autoregressive Generation)\n",
    "def translate_sentence(input_sentence):\n",
    "    encoder_input = np.array([[eng_vocab[word] for word in input_sentence]])  # Convert words to indices\n",
    "    encoder_input_pos = np.tile(PE_eng, (1, 1, 1))  # Use fixed PE\n",
    "\n",
    "    decoder_input = np.zeros((1, target_seq_len))  # Start with empty decoder input\n",
    "    decoder_input_pos = np.tile(PE_chn, (1, 1, 1))  # Use fixed PE\n",
    "\n",
    "    translated_sentence = []\n",
    "    \n",
    "    for i in range(target_seq_len):\n",
    "        pred_probs = model.predict([encoder_input, encoder_input_pos, decoder_input, decoder_input_pos])[0, i, :]\n",
    "        predicted_word_index = np.argmax(pred_probs)  # Select highest probability word\n",
    "        translated_word = list(chn_vocab.keys())[list(chn_vocab.values()).index(predicted_word_index)]\n",
    "        \n",
    "        translated_sentence.append(translated_word)\n",
    "        decoder_input[0, i] = predicted_word_index  # Feed output back to decoder\n",
    "\n",
    "    return \" \".join(translated_sentence)\n",
    "\n",
    "# 🔟 Run Translation\n",
    "print(\"Predicted Translation:\", translate_sentence([\"I\", \"love\", \"cats\"]))  # Expected: \"我 喜 歡 貓\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentimental analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Dense, Layer, Flatten\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "VOCAB_SIZE = 20  # 20 unique tokens\n",
    "EMBED_DIM = 4    # Each word is a 4D vector\n",
    "SEQ_LENGTH = 3   # Short sequence length\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "# Dummy dataset (random sequences and binary labels)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randint(0, VOCAB_SIZE, size=(1000, SEQ_LENGTH))\n",
    "y_train = np.random.randint(0, 2, size=(1000, 1))\n",
    "\n",
    "X_val = np.random.randint(0, VOCAB_SIZE, size=(200, SEQ_LENGTH))\n",
    "y_val = np.random.randint(0, 2, size=(200, 1))\n",
    "\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        # Learnable weight matrices for Query, Key, and Value\n",
    "        self.W_q = Dense(embed_dim, use_bias=False)\n",
    "        self.W_k = Dense(embed_dim, use_bias=False)\n",
    "        self.W_v = Dense(embed_dim, use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Compute Q, K, V matrices\n",
    "        Q = self.W_q(inputs)  # (batch_size, seq_length, embed_dim)\n",
    "        K = self.W_k(inputs)  # (batch_size, seq_length, embed_dim)\n",
    "        V = self.W_v(inputs)  # (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)  # (batch_size, seq_length, seq_length)\n",
    "        output = tf.matmul(attention_weights, V)  # (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Define model\n",
    "inputs = keras.Input(shape=(SEQ_LENGTH,))\n",
    "\n",
    "# Embedding Layer\n",
    "embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM)(inputs)\n",
    "\n",
    "# Self-Attention Layer\n",
    "attention_output = SelfAttention(EMBED_DIM)(embedding)\n",
    "\n",
    "# Flatten the output (or use pooling instead)\n",
    "flattened = Flatten()(attention_output)\n",
    "\n",
    "# Fully Connected Layer\n",
    "fc_output = Dense(1, activation='sigmoid')(flattened)\n",
    "\n",
    "# Compile the model\n",
    "model = keras.Model(inputs=inputs, outputs=fc_output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "test_sentence = np.array([[3, 5, 7]])  # Example sequence of token indices\n",
    "predicted_sentiment = model.predict(test_sentence)\n",
    "print(\"Predicted Sentiment Score:\", predicted_sentiment[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)           │            <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention (\u001b[38;5;33mSelfAttention\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m4\u001b[0m)           │            \u001b[38;5;34m48\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m40\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m9\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">177</span> (708.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m177\u001b[0m (708.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">177</span> (708.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m177\u001b[0m (708.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5100 - loss: 0.6932 - val_accuracy: 0.4650 - val_loss: 0.6941\n",
      "Epoch 2/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4995 - loss: 0.6930 - val_accuracy: 0.4600 - val_loss: 0.6946\n",
      "Epoch 3/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4997 - loss: 0.6929 - val_accuracy: 0.4600 - val_loss: 0.6951\n",
      "Epoch 4/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5218 - loss: 0.6917 - val_accuracy: 0.4600 - val_loss: 0.6960\n",
      "Epoch 5/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5105 - loss: 0.6922 - val_accuracy: 0.4600 - val_loss: 0.6964\n",
      "Epoch 6/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5028 - loss: 0.6919 - val_accuracy: 0.4550 - val_loss: 0.6962\n",
      "Epoch 7/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5067 - loss: 0.6911 - val_accuracy: 0.4450 - val_loss: 0.6975\n",
      "Epoch 8/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5130 - loss: 0.6921 - val_accuracy: 0.4600 - val_loss: 0.6971\n",
      "Epoch 9/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5351 - loss: 0.6891 - val_accuracy: 0.4650 - val_loss: 0.6986\n",
      "Epoch 10/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5014 - loss: 0.6927 - val_accuracy: 0.4450 - val_loss: 0.6988\n",
      "Epoch 11/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5636 - loss: 0.6856 - val_accuracy: 0.4400 - val_loss: 0.7010\n",
      "Epoch 12/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5489 - loss: 0.6869 - val_accuracy: 0.4700 - val_loss: 0.7017\n",
      "Epoch 13/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5231 - loss: 0.6879 - val_accuracy: 0.4850 - val_loss: 0.7014\n",
      "Epoch 14/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5757 - loss: 0.6844 - val_accuracy: 0.4800 - val_loss: 0.7030\n",
      "Epoch 15/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5669 - loss: 0.6839 - val_accuracy: 0.4750 - val_loss: 0.7031\n",
      "Epoch 16/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5437 - loss: 0.6855 - val_accuracy: 0.4800 - val_loss: 0.7034\n",
      "Epoch 17/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5535 - loss: 0.6825 - val_accuracy: 0.4700 - val_loss: 0.7059\n",
      "Epoch 18/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5673 - loss: 0.6812 - val_accuracy: 0.4850 - val_loss: 0.7062\n",
      "Epoch 19/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5529 - loss: 0.6832 - val_accuracy: 0.4850 - val_loss: 0.7081\n",
      "Epoch 20/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5856 - loss: 0.6787 - val_accuracy: 0.4750 - val_loss: 0.7086\n",
      "Epoch 21/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5816 - loss: 0.6790 - val_accuracy: 0.4950 - val_loss: 0.7090\n",
      "Epoch 22/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5670 - loss: 0.6811 - val_accuracy: 0.4950 - val_loss: 0.7084\n",
      "Epoch 23/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5821 - loss: 0.6719 - val_accuracy: 0.5000 - val_loss: 0.7095\n",
      "Epoch 24/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5812 - loss: 0.6778 - val_accuracy: 0.4900 - val_loss: 0.7092\n",
      "Epoch 25/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5991 - loss: 0.6729 - val_accuracy: 0.4900 - val_loss: 0.7101\n",
      "Epoch 26/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5582 - loss: 0.6823 - val_accuracy: 0.5000 - val_loss: 0.7100\n",
      "Epoch 27/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5877 - loss: 0.6777 - val_accuracy: 0.4850 - val_loss: 0.7091\n",
      "Epoch 28/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5611 - loss: 0.6774 - val_accuracy: 0.4900 - val_loss: 0.7107\n",
      "Epoch 29/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5753 - loss: 0.6790 - val_accuracy: 0.4900 - val_loss: 0.7108\n",
      "Epoch 30/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5706 - loss: 0.6796 - val_accuracy: 0.4900 - val_loss: 0.7110\n",
      "Epoch 31/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5563 - loss: 0.6831 - val_accuracy: 0.4950 - val_loss: 0.7116\n",
      "Epoch 32/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5790 - loss: 0.6740 - val_accuracy: 0.5050 - val_loss: 0.7115\n",
      "Epoch 33/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5826 - loss: 0.6784 - val_accuracy: 0.5000 - val_loss: 0.7108\n",
      "Epoch 34/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5815 - loss: 0.6764 - val_accuracy: 0.5050 - val_loss: 0.7123\n",
      "Epoch 35/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5921 - loss: 0.6790 - val_accuracy: 0.5100 - val_loss: 0.7100\n",
      "Epoch 36/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5901 - loss: 0.6779 - val_accuracy: 0.5100 - val_loss: 0.7112\n",
      "Epoch 37/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5912 - loss: 0.6728 - val_accuracy: 0.5150 - val_loss: 0.7120\n",
      "Epoch 38/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5625 - loss: 0.6782 - val_accuracy: 0.5150 - val_loss: 0.7109\n",
      "Epoch 39/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5868 - loss: 0.6716 - val_accuracy: 0.5100 - val_loss: 0.7112\n",
      "Epoch 40/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5794 - loss: 0.6774 - val_accuracy: 0.5200 - val_loss: 0.7128\n",
      "Epoch 41/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5929 - loss: 0.6770 - val_accuracy: 0.5100 - val_loss: 0.7108\n",
      "Epoch 42/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5974 - loss: 0.6741 - val_accuracy: 0.5100 - val_loss: 0.7119\n",
      "Epoch 43/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6193 - loss: 0.6668 - val_accuracy: 0.5050 - val_loss: 0.7117\n",
      "Epoch 44/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6106 - loss: 0.6746 - val_accuracy: 0.5050 - val_loss: 0.7130\n",
      "Epoch 45/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5908 - loss: 0.6731 - val_accuracy: 0.5050 - val_loss: 0.7137\n",
      "Epoch 46/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5961 - loss: 0.6714 - val_accuracy: 0.5050 - val_loss: 0.7151\n",
      "Epoch 47/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5890 - loss: 0.6734 - val_accuracy: 0.5000 - val_loss: 0.7147\n",
      "Epoch 48/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6003 - loss: 0.6672 - val_accuracy: 0.5000 - val_loss: 0.7155\n",
      "Epoch 49/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6240 - loss: 0.6646 - val_accuracy: 0.5050 - val_loss: 0.7143\n",
      "Epoch 50/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5958 - loss: 0.6719 - val_accuracy: 0.5150 - val_loss: 0.7140\n",
      "Epoch 51/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5973 - loss: 0.6664 - val_accuracy: 0.4850 - val_loss: 0.7162\n",
      "Epoch 52/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5985 - loss: 0.6696 - val_accuracy: 0.4950 - val_loss: 0.7162\n",
      "Epoch 53/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5937 - loss: 0.6660 - val_accuracy: 0.4950 - val_loss: 0.7162\n",
      "Epoch 54/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5795 - loss: 0.6670 - val_accuracy: 0.4800 - val_loss: 0.7188\n",
      "Epoch 55/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5698 - loss: 0.6755 - val_accuracy: 0.4750 - val_loss: 0.7202\n",
      "Epoch 56/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5746 - loss: 0.6694 - val_accuracy: 0.4500 - val_loss: 0.7176\n",
      "Epoch 57/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5834 - loss: 0.6612 - val_accuracy: 0.4750 - val_loss: 0.7189\n",
      "Epoch 58/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5944 - loss: 0.6599 - val_accuracy: 0.4550 - val_loss: 0.7211\n",
      "Epoch 59/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5957 - loss: 0.6668 - val_accuracy: 0.4550 - val_loss: 0.7206\n",
      "Epoch 60/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5778 - loss: 0.6751 - val_accuracy: 0.4650 - val_loss: 0.7188\n",
      "Epoch 61/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5895 - loss: 0.6664 - val_accuracy: 0.4600 - val_loss: 0.7217\n",
      "Epoch 62/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5856 - loss: 0.6682 - val_accuracy: 0.4400 - val_loss: 0.7208\n",
      "Epoch 63/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6285 - loss: 0.6510 - val_accuracy: 0.4650 - val_loss: 0.7207\n",
      "Epoch 64/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5937 - loss: 0.6630 - val_accuracy: 0.4400 - val_loss: 0.7223\n",
      "Epoch 65/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6267 - loss: 0.6522 - val_accuracy: 0.4650 - val_loss: 0.7218\n",
      "Epoch 66/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6048 - loss: 0.6581 - val_accuracy: 0.4450 - val_loss: 0.7251\n",
      "Epoch 67/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5994 - loss: 0.6696 - val_accuracy: 0.4450 - val_loss: 0.7242\n",
      "Epoch 68/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5860 - loss: 0.6682 - val_accuracy: 0.4500 - val_loss: 0.7261\n",
      "Epoch 69/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5926 - loss: 0.6615 - val_accuracy: 0.4550 - val_loss: 0.7257\n",
      "Epoch 70/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5795 - loss: 0.6679 - val_accuracy: 0.4600 - val_loss: 0.7258\n",
      "Epoch 71/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5936 - loss: 0.6572 - val_accuracy: 0.4700 - val_loss: 0.7268\n",
      "Epoch 72/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5791 - loss: 0.6687 - val_accuracy: 0.4700 - val_loss: 0.7259\n",
      "Epoch 73/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6038 - loss: 0.6564 - val_accuracy: 0.4700 - val_loss: 0.7260\n",
      "Epoch 74/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5975 - loss: 0.6602 - val_accuracy: 0.4600 - val_loss: 0.7280\n",
      "Epoch 75/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6275 - loss: 0.6554 - val_accuracy: 0.4500 - val_loss: 0.7287\n",
      "Epoch 76/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5863 - loss: 0.6636 - val_accuracy: 0.4550 - val_loss: 0.7278\n",
      "Epoch 77/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5890 - loss: 0.6657 - val_accuracy: 0.4650 - val_loss: 0.7280\n",
      "Epoch 78/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5864 - loss: 0.6677 - val_accuracy: 0.4550 - val_loss: 0.7290\n",
      "Epoch 79/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5867 - loss: 0.6691 - val_accuracy: 0.4600 - val_loss: 0.7267\n",
      "Epoch 80/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5956 - loss: 0.6674 - val_accuracy: 0.4550 - val_loss: 0.7275\n",
      "Epoch 81/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5637 - loss: 0.6695 - val_accuracy: 0.4600 - val_loss: 0.7278\n",
      "Epoch 82/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6117 - loss: 0.6541 - val_accuracy: 0.4600 - val_loss: 0.7274\n",
      "Epoch 83/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6041 - loss: 0.6608 - val_accuracy: 0.4600 - val_loss: 0.7285\n",
      "Epoch 84/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6015 - loss: 0.6599 - val_accuracy: 0.4700 - val_loss: 0.7261\n",
      "Epoch 85/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6044 - loss: 0.6550 - val_accuracy: 0.4700 - val_loss: 0.7293\n",
      "Epoch 86/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6218 - loss: 0.6509 - val_accuracy: 0.4450 - val_loss: 0.7285\n",
      "Epoch 87/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5953 - loss: 0.6631 - val_accuracy: 0.4800 - val_loss: 0.7259\n",
      "Epoch 88/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5726 - loss: 0.6671 - val_accuracy: 0.4750 - val_loss: 0.7278\n",
      "Epoch 89/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6033 - loss: 0.6613 - val_accuracy: 0.4650 - val_loss: 0.7296\n",
      "Epoch 90/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5882 - loss: 0.6641 - val_accuracy: 0.4700 - val_loss: 0.7272\n",
      "Epoch 91/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6271 - loss: 0.6475 - val_accuracy: 0.4500 - val_loss: 0.7295\n",
      "Epoch 92/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5960 - loss: 0.6617 - val_accuracy: 0.4600 - val_loss: 0.7272\n",
      "Epoch 93/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5977 - loss: 0.6606 - val_accuracy: 0.4700 - val_loss: 0.7279\n",
      "Epoch 94/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6012 - loss: 0.6499 - val_accuracy: 0.4600 - val_loss: 0.7290\n",
      "Epoch 95/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6166 - loss: 0.6552 - val_accuracy: 0.4600 - val_loss: 0.7289\n",
      "Epoch 96/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5975 - loss: 0.6572 - val_accuracy: 0.4550 - val_loss: 0.7286\n",
      "Epoch 97/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5879 - loss: 0.6674 - val_accuracy: 0.4600 - val_loss: 0.7297\n",
      "Epoch 98/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5850 - loss: 0.6725 - val_accuracy: 0.4600 - val_loss: 0.7281\n",
      "Epoch 99/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6142 - loss: 0.6515 - val_accuracy: 0.4500 - val_loss: 0.7300\n",
      "Epoch 100/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5965 - loss: 0.6600 - val_accuracy: 0.4750 - val_loss: 0.7275\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "Predicted Sentiment Score: 0.5758915\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Dense, Layer, GlobalAveragePooling1D\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "VOCAB_SIZE = 20   # Number of unique tokens\n",
    "EMBED_DIM = 4     # Dimension of word embeddings\n",
    "SEQ_LENGTH = 3    # Number of words per sentence\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "# Dummy dataset (random sequences and binary labels)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randint(0, VOCAB_SIZE, size=(1000, SEQ_LENGTH))\n",
    "y_train = np.random.randint(0, 2, size=(1000, 1))\n",
    "\n",
    "X_val = np.random.randint(0, VOCAB_SIZE, size=(200, SEQ_LENGTH))\n",
    "y_val = np.random.randint(0, 2, size=(200, 1))\n",
    "\n",
    "\n",
    "# Define Self-Attention Layer\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.W_q = Dense(embed_dim, use_bias=False)\n",
    "        self.W_k = Dense(embed_dim, use_bias=False)\n",
    "        self.W_v = Dense(embed_dim, use_bias=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Q = self.W_q(inputs)\n",
    "        K = self.W_k(inputs)\n",
    "        V = self.W_v(inputs)\n",
    "\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        output = tf.matmul(attention_weights, V)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Define Model\n",
    "inputs = keras.Input(shape=(SEQ_LENGTH,))\n",
    "\n",
    "# Trainable Embedding Layer (Joint Training)\n",
    "embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM, trainable=True)(inputs)\n",
    "\n",
    "# Self-Attention Layer\n",
    "attention_output = SelfAttention(EMBED_DIM)(embedding)\n",
    "\n",
    "# Global Average Pooling (better than Flatten for small models)\n",
    "pooled_output = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "# Fully Connected Layer\n",
    "fc_output = Dense(8, activation='relu')(pooled_output)\n",
    "fc_output = Dense(1, activation='sigmoid')(fc_output)\n",
    "\n",
    "# Compile Model\n",
    "model = keras.Model(inputs=inputs, outputs=fc_output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Test Prediction\n",
    "test_sentence = np.array([[3, 5, 7]])  # Example sequence of token indices\n",
    "predicted_sentiment = model.predict(test_sentence)\n",
    "print(\"Predicted Sentiment Score:\", predicted_sentiment[0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6051 - loss: 0.6603 - val_accuracy: 0.4750 - val_loss: 0.7292\n",
      "Epoch 2/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5984 - loss: 0.6615 - val_accuracy: 0.4750 - val_loss: 0.7297\n",
      "Epoch 3/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6031 - loss: 0.6552 - val_accuracy: 0.4550 - val_loss: 0.7296\n",
      "Epoch 4/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5809 - loss: 0.6701 - val_accuracy: 0.4650 - val_loss: 0.7289\n",
      "Epoch 5/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6171 - loss: 0.6493 - val_accuracy: 0.4700 - val_loss: 0.7301\n",
      "Epoch 6/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5806 - loss: 0.6658 - val_accuracy: 0.4700 - val_loss: 0.7313\n",
      "Epoch 7/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6269 - loss: 0.6481 - val_accuracy: 0.4700 - val_loss: 0.7333\n",
      "Epoch 8/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6081 - loss: 0.6448 - val_accuracy: 0.4650 - val_loss: 0.7318\n",
      "Epoch 9/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5911 - loss: 0.6612 - val_accuracy: 0.4750 - val_loss: 0.7304\n",
      "Epoch 10/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5847 - loss: 0.6603 - val_accuracy: 0.4750 - val_loss: 0.7289\n",
      "Epoch 11/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6049 - loss: 0.6569 - val_accuracy: 0.4700 - val_loss: 0.7308\n",
      "Epoch 12/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6138 - loss: 0.6543 - val_accuracy: 0.4750 - val_loss: 0.7317\n",
      "Epoch 13/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6214 - loss: 0.6482 - val_accuracy: 0.4850 - val_loss: 0.7318\n",
      "Epoch 14/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6265 - loss: 0.6446 - val_accuracy: 0.4750 - val_loss: 0.7305\n",
      "Epoch 15/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5996 - loss: 0.6571 - val_accuracy: 0.4850 - val_loss: 0.7305\n",
      "Epoch 16/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6061 - loss: 0.6523 - val_accuracy: 0.4650 - val_loss: 0.7301\n",
      "Epoch 17/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5895 - loss: 0.6619 - val_accuracy: 0.4700 - val_loss: 0.7288\n",
      "Epoch 18/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6214 - loss: 0.6556 - val_accuracy: 0.4850 - val_loss: 0.7297\n",
      "Epoch 19/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5940 - loss: 0.6603 - val_accuracy: 0.4800 - val_loss: 0.7319\n",
      "Epoch 20/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6088 - loss: 0.6620 - val_accuracy: 0.4700 - val_loss: 0.7300\n",
      "Epoch 21/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6097 - loss: 0.6476 - val_accuracy: 0.4800 - val_loss: 0.7305\n",
      "Epoch 22/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6081 - loss: 0.6483 - val_accuracy: 0.4800 - val_loss: 0.7314\n",
      "Epoch 23/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6125 - loss: 0.6573 - val_accuracy: 0.4850 - val_loss: 0.7315\n",
      "Epoch 24/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5976 - loss: 0.6608 - val_accuracy: 0.4750 - val_loss: 0.7310\n",
      "Epoch 25/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6012 - loss: 0.6654 - val_accuracy: 0.4800 - val_loss: 0.7301\n",
      "Epoch 26/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5980 - loss: 0.6565 - val_accuracy: 0.4750 - val_loss: 0.7318\n",
      "Epoch 27/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6146 - loss: 0.6582 - val_accuracy: 0.4750 - val_loss: 0.7300\n",
      "Epoch 28/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5890 - loss: 0.6655 - val_accuracy: 0.4800 - val_loss: 0.7287\n",
      "Epoch 29/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5824 - loss: 0.6617 - val_accuracy: 0.4750 - val_loss: 0.7301\n",
      "Epoch 30/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6095 - loss: 0.6549 - val_accuracy: 0.4750 - val_loss: 0.7304\n",
      "Epoch 31/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6023 - loss: 0.6575 - val_accuracy: 0.4750 - val_loss: 0.7337\n",
      "Epoch 32/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6177 - loss: 0.6524 - val_accuracy: 0.4750 - val_loss: 0.7321\n",
      "Epoch 33/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6405 - loss: 0.6435 - val_accuracy: 0.4750 - val_loss: 0.7343\n",
      "Epoch 34/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6160 - loss: 0.6596 - val_accuracy: 0.4800 - val_loss: 0.7316\n",
      "Epoch 35/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5933 - loss: 0.6533 - val_accuracy: 0.4850 - val_loss: 0.7318\n",
      "Epoch 36/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5980 - loss: 0.6651 - val_accuracy: 0.4800 - val_loss: 0.7302\n",
      "Epoch 37/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5883 - loss: 0.6646 - val_accuracy: 0.4800 - val_loss: 0.7316\n",
      "Epoch 38/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6247 - loss: 0.6533 - val_accuracy: 0.4750 - val_loss: 0.7316\n",
      "Epoch 39/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6041 - loss: 0.6577 - val_accuracy: 0.4800 - val_loss: 0.7333\n",
      "Epoch 40/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5778 - loss: 0.6693 - val_accuracy: 0.4850 - val_loss: 0.7342\n",
      "Epoch 41/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6078 - loss: 0.6575 - val_accuracy: 0.4800 - val_loss: 0.7349\n",
      "Epoch 42/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6220 - loss: 0.6534 - val_accuracy: 0.4850 - val_loss: 0.7345\n",
      "Epoch 43/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5925 - loss: 0.6740 - val_accuracy: 0.4750 - val_loss: 0.7336\n",
      "Epoch 44/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6151 - loss: 0.6502 - val_accuracy: 0.4800 - val_loss: 0.7368\n",
      "Epoch 45/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6325 - loss: 0.6529 - val_accuracy: 0.4900 - val_loss: 0.7355\n",
      "Epoch 46/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6499 - loss: 0.6372 - val_accuracy: 0.4850 - val_loss: 0.7342\n",
      "Epoch 47/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5943 - loss: 0.6648 - val_accuracy: 0.4900 - val_loss: 0.7335\n",
      "Epoch 48/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6211 - loss: 0.6556 - val_accuracy: 0.4900 - val_loss: 0.7349\n",
      "Epoch 49/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6222 - loss: 0.6478 - val_accuracy: 0.4800 - val_loss: 0.7342\n",
      "Epoch 50/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5781 - loss: 0.6742 - val_accuracy: 0.4850 - val_loss: 0.7366\n",
      "Epoch 51/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6188 - loss: 0.6460 - val_accuracy: 0.4800 - val_loss: 0.7354\n",
      "Epoch 52/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6056 - loss: 0.6583 - val_accuracy: 0.4850 - val_loss: 0.7363\n",
      "Epoch 53/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6097 - loss: 0.6611 - val_accuracy: 0.4800 - val_loss: 0.7331\n",
      "Epoch 54/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5784 - loss: 0.6592 - val_accuracy: 0.4900 - val_loss: 0.7325\n",
      "Epoch 55/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6372 - loss: 0.6466 - val_accuracy: 0.4800 - val_loss: 0.7335\n",
      "Epoch 56/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5850 - loss: 0.6597 - val_accuracy: 0.4900 - val_loss: 0.7316\n",
      "Epoch 57/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6111 - loss: 0.6517 - val_accuracy: 0.4950 - val_loss: 0.7331\n",
      "Epoch 58/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6125 - loss: 0.6572 - val_accuracy: 0.4800 - val_loss: 0.7350\n",
      "Epoch 59/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6182 - loss: 0.6543 - val_accuracy: 0.4850 - val_loss: 0.7350\n",
      "Epoch 60/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6195 - loss: 0.6410 - val_accuracy: 0.4750 - val_loss: 0.7356\n",
      "Epoch 61/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6137 - loss: 0.6548 - val_accuracy: 0.4850 - val_loss: 0.7365\n",
      "Epoch 62/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5961 - loss: 0.6642 - val_accuracy: 0.4850 - val_loss: 0.7361\n",
      "Epoch 63/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6131 - loss: 0.6580 - val_accuracy: 0.4800 - val_loss: 0.7350\n",
      "Epoch 64/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5860 - loss: 0.6706 - val_accuracy: 0.4850 - val_loss: 0.7347\n",
      "Epoch 65/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6034 - loss: 0.6556 - val_accuracy: 0.4850 - val_loss: 0.7342\n",
      "Epoch 66/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5693 - loss: 0.6644 - val_accuracy: 0.4850 - val_loss: 0.7360\n",
      "Epoch 67/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6136 - loss: 0.6484 - val_accuracy: 0.4800 - val_loss: 0.7346\n",
      "Epoch 68/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.6063 - loss: 0.6500 - val_accuracy: 0.4850 - val_loss: 0.7362\n",
      "Epoch 69/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6165 - loss: 0.6557 - val_accuracy: 0.4950 - val_loss: 0.7357\n",
      "Epoch 70/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5857 - loss: 0.6596 - val_accuracy: 0.4950 - val_loss: 0.7371\n",
      "Epoch 71/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5960 - loss: 0.6615 - val_accuracy: 0.4850 - val_loss: 0.7366\n",
      "Epoch 72/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6173 - loss: 0.6468 - val_accuracy: 0.4900 - val_loss: 0.7355\n",
      "Epoch 73/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5899 - loss: 0.6593 - val_accuracy: 0.5000 - val_loss: 0.7328\n",
      "Epoch 74/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5897 - loss: 0.6620 - val_accuracy: 0.5050 - val_loss: 0.7366\n",
      "Epoch 75/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6169 - loss: 0.6494 - val_accuracy: 0.4950 - val_loss: 0.7355\n",
      "Epoch 76/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6011 - loss: 0.6592 - val_accuracy: 0.4900 - val_loss: 0.7357\n",
      "Epoch 77/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6115 - loss: 0.6419 - val_accuracy: 0.5050 - val_loss: 0.7340\n",
      "Epoch 78/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6087 - loss: 0.6555 - val_accuracy: 0.5050 - val_loss: 0.7348\n",
      "Epoch 79/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5836 - loss: 0.6562 - val_accuracy: 0.5000 - val_loss: 0.7346\n",
      "Epoch 80/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6036 - loss: 0.6525 - val_accuracy: 0.4950 - val_loss: 0.7342\n",
      "Epoch 81/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6081 - loss: 0.6477 - val_accuracy: 0.4800 - val_loss: 0.7340\n",
      "Epoch 82/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5923 - loss: 0.6575 - val_accuracy: 0.4900 - val_loss: 0.7349\n",
      "Epoch 83/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5896 - loss: 0.6583 - val_accuracy: 0.4900 - val_loss: 0.7349\n",
      "Epoch 84/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5986 - loss: 0.6533 - val_accuracy: 0.4850 - val_loss: 0.7372\n",
      "Epoch 85/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6190 - loss: 0.6459 - val_accuracy: 0.4800 - val_loss: 0.7371\n",
      "Epoch 86/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6146 - loss: 0.6438 - val_accuracy: 0.4850 - val_loss: 0.7379\n",
      "Epoch 87/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6077 - loss: 0.6459 - val_accuracy: 0.4850 - val_loss: 0.7355\n",
      "Epoch 88/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6100 - loss: 0.6480 - val_accuracy: 0.4750 - val_loss: 0.7380\n",
      "Epoch 89/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5885 - loss: 0.6655 - val_accuracy: 0.4850 - val_loss: 0.7345\n",
      "Epoch 90/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6212 - loss: 0.6360 - val_accuracy: 0.4850 - val_loss: 0.7342\n",
      "Epoch 91/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5933 - loss: 0.6543 - val_accuracy: 0.4900 - val_loss: 0.7344\n",
      "Epoch 92/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5991 - loss: 0.6540 - val_accuracy: 0.4900 - val_loss: 0.7340\n",
      "Epoch 93/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5953 - loss: 0.6498 - val_accuracy: 0.4800 - val_loss: 0.7347\n",
      "Epoch 94/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6320 - loss: 0.6420 - val_accuracy: 0.4900 - val_loss: 0.7382\n",
      "Epoch 95/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6070 - loss: 0.6400 - val_accuracy: 0.5050 - val_loss: 0.7369\n",
      "Epoch 96/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6025 - loss: 0.6535 - val_accuracy: 0.4850 - val_loss: 0.7373\n",
      "Epoch 97/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6072 - loss: 0.6488 - val_accuracy: 0.4650 - val_loss: 0.7370\n",
      "Epoch 98/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5950 - loss: 0.6703 - val_accuracy: 0.4650 - val_loss: 0.7365\n",
      "Epoch 99/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6129 - loss: 0.6421 - val_accuracy: 0.4900 - val_loss: 0.7403\n",
      "Epoch 100/100\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6376 - loss: 0.6401 - val_accuracy: 0.4850 - val_loss: 0.7381\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train Model and store training history\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Plot Accuracy and Loss Curves\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    axes[0].set_title('Model Accuracy')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot loss\n",
    "    axes[1].plot(history.history['loss'], label='Training Loss')\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
    "    axes[1].set_title('Model Loss')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to display the graphs\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_4                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SelfAttention</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_8 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_6 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m8\u001b[0m)           │           \u001b[38;5;34m160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ self_attention_4                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m8\u001b[0m)           │           \u001b[38;5;34m208\u001b[0m │\n",
       "│ (\u001b[38;5;33mSelfAttention\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">529</span> (2.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m529\u001b[0m (2.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">529</span> (2.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m529\u001b[0m (2.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.4979 - loss: 0.7053 - val_accuracy: 0.4950 - val_loss: 0.7047\n",
      "Epoch 2/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.4987 - loss: 0.6976 - val_accuracy: 0.4750 - val_loss: 0.7004\n",
      "Epoch 3/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5711 - loss: 0.6832 - val_accuracy: 0.4350 - val_loss: 0.7053\n",
      "Epoch 4/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5697 - loss: 0.6831 - val_accuracy: 0.4050 - val_loss: 0.7080\n",
      "Epoch 5/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5802 - loss: 0.6809 - val_accuracy: 0.4450 - val_loss: 0.7082\n",
      "Epoch 6/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5637 - loss: 0.6807 - val_accuracy: 0.4600 - val_loss: 0.7082\n",
      "Epoch 7/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5882 - loss: 0.6784 - val_accuracy: 0.4550 - val_loss: 0.7063\n",
      "Epoch 8/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5969 - loss: 0.6762 - val_accuracy: 0.4850 - val_loss: 0.7035\n",
      "Epoch 9/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5851 - loss: 0.6749 - val_accuracy: 0.4650 - val_loss: 0.7046\n",
      "Epoch 10/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5717 - loss: 0.6794 - val_accuracy: 0.4800 - val_loss: 0.7058\n",
      "Epoch 11/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5721 - loss: 0.6759 - val_accuracy: 0.4800 - val_loss: 0.7071\n",
      "Epoch 12/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5744 - loss: 0.6788 - val_accuracy: 0.5150 - val_loss: 0.7030\n",
      "Epoch 13/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6039 - loss: 0.6709 - val_accuracy: 0.5200 - val_loss: 0.7044\n",
      "Epoch 14/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6128 - loss: 0.6696 - val_accuracy: 0.5250 - val_loss: 0.7016\n",
      "Epoch 15/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6089 - loss: 0.6640 - val_accuracy: 0.5250 - val_loss: 0.7039\n",
      "Epoch 16/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5936 - loss: 0.6694 - val_accuracy: 0.5400 - val_loss: 0.7017\n",
      "Epoch 17/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5840 - loss: 0.6700 - val_accuracy: 0.5350 - val_loss: 0.6993\n",
      "Epoch 18/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6225 - loss: 0.6592 - val_accuracy: 0.5300 - val_loss: 0.7066\n",
      "Epoch 19/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6139 - loss: 0.6528 - val_accuracy: 0.5150 - val_loss: 0.7014\n",
      "Epoch 20/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6173 - loss: 0.6600 - val_accuracy: 0.5200 - val_loss: 0.7053\n",
      "Epoch 21/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6069 - loss: 0.6673 - val_accuracy: 0.5000 - val_loss: 0.7038\n",
      "Epoch 22/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5764 - loss: 0.6689 - val_accuracy: 0.5200 - val_loss: 0.7049\n",
      "Epoch 23/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5969 - loss: 0.6603 - val_accuracy: 0.5200 - val_loss: 0.7047\n",
      "Epoch 24/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5884 - loss: 0.6609 - val_accuracy: 0.4950 - val_loss: 0.7092\n",
      "Epoch 25/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6240 - loss: 0.6589 - val_accuracy: 0.4800 - val_loss: 0.7091\n",
      "Epoch 26/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6097 - loss: 0.6604 - val_accuracy: 0.4750 - val_loss: 0.7110\n",
      "Epoch 27/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6017 - loss: 0.6636 - val_accuracy: 0.5000 - val_loss: 0.7127\n",
      "Epoch 28/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6265 - loss: 0.6553 - val_accuracy: 0.5100 - val_loss: 0.7080\n",
      "Epoch 29/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6057 - loss: 0.6537 - val_accuracy: 0.5100 - val_loss: 0.7067\n",
      "Epoch 30/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6462 - loss: 0.6565 - val_accuracy: 0.4850 - val_loss: 0.7104\n",
      "Epoch 31/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6127 - loss: 0.6563 - val_accuracy: 0.5000 - val_loss: 0.7121\n",
      "Epoch 32/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6181 - loss: 0.6577 - val_accuracy: 0.4700 - val_loss: 0.7130\n",
      "Epoch 33/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6171 - loss: 0.6568 - val_accuracy: 0.4750 - val_loss: 0.7143\n",
      "Epoch 34/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5938 - loss: 0.6639 - val_accuracy: 0.4750 - val_loss: 0.7174\n",
      "Epoch 35/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5972 - loss: 0.6682 - val_accuracy: 0.4800 - val_loss: 0.7181\n",
      "Epoch 36/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6510 - loss: 0.6435 - val_accuracy: 0.5000 - val_loss: 0.7087\n",
      "Epoch 37/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6098 - loss: 0.6534 - val_accuracy: 0.5050 - val_loss: 0.7152\n",
      "Epoch 38/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6138 - loss: 0.6523 - val_accuracy: 0.5150 - val_loss: 0.7146\n",
      "Epoch 39/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6346 - loss: 0.6605 - val_accuracy: 0.5000 - val_loss: 0.7151\n",
      "Epoch 40/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6451 - loss: 0.6502 - val_accuracy: 0.5250 - val_loss: 0.7104\n",
      "Epoch 41/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6327 - loss: 0.6564 - val_accuracy: 0.5200 - val_loss: 0.7145\n",
      "Epoch 42/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6317 - loss: 0.6488 - val_accuracy: 0.5050 - val_loss: 0.7149\n",
      "Epoch 43/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6263 - loss: 0.6518 - val_accuracy: 0.5350 - val_loss: 0.7141\n",
      "Epoch 44/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6243 - loss: 0.6439 - val_accuracy: 0.5150 - val_loss: 0.7145\n",
      "Epoch 45/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6248 - loss: 0.6572 - val_accuracy: 0.5250 - val_loss: 0.7150\n",
      "Epoch 46/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6532 - loss: 0.6412 - val_accuracy: 0.5000 - val_loss: 0.7213\n",
      "Epoch 47/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6288 - loss: 0.6508 - val_accuracy: 0.5250 - val_loss: 0.7145\n",
      "Epoch 48/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6495 - loss: 0.6401 - val_accuracy: 0.5150 - val_loss: 0.7182\n",
      "Epoch 49/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6484 - loss: 0.6421 - val_accuracy: 0.5250 - val_loss: 0.7173\n",
      "Epoch 50/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6346 - loss: 0.6447 - val_accuracy: 0.5150 - val_loss: 0.7192\n",
      "Epoch 51/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6108 - loss: 0.6603 - val_accuracy: 0.5100 - val_loss: 0.7197\n",
      "Epoch 52/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6241 - loss: 0.6525 - val_accuracy: 0.5200 - val_loss: 0.7190\n",
      "Epoch 53/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6611 - loss: 0.6337 - val_accuracy: 0.5250 - val_loss: 0.7196\n",
      "Epoch 54/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6728 - loss: 0.6424 - val_accuracy: 0.5050 - val_loss: 0.7219\n",
      "Epoch 55/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6374 - loss: 0.6494 - val_accuracy: 0.5300 - val_loss: 0.7211\n",
      "Epoch 56/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6328 - loss: 0.6422 - val_accuracy: 0.5000 - val_loss: 0.7250\n",
      "Epoch 57/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6336 - loss: 0.6454 - val_accuracy: 0.5100 - val_loss: 0.7232\n",
      "Epoch 58/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6212 - loss: 0.6512 - val_accuracy: 0.5450 - val_loss: 0.7223\n",
      "Epoch 59/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6411 - loss: 0.6445 - val_accuracy: 0.5300 - val_loss: 0.7276\n",
      "Epoch 60/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6323 - loss: 0.6503 - val_accuracy: 0.5200 - val_loss: 0.7279\n",
      "Epoch 61/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6423 - loss: 0.6325 - val_accuracy: 0.5300 - val_loss: 0.7207\n",
      "Epoch 62/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6543 - loss: 0.6383 - val_accuracy: 0.5250 - val_loss: 0.7263\n",
      "Epoch 63/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6777 - loss: 0.6224 - val_accuracy: 0.5100 - val_loss: 0.7232\n",
      "Epoch 64/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6486 - loss: 0.6344 - val_accuracy: 0.5150 - val_loss: 0.7269\n",
      "Epoch 65/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6524 - loss: 0.6455 - val_accuracy: 0.5000 - val_loss: 0.7349\n",
      "Epoch 66/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6677 - loss: 0.6260 - val_accuracy: 0.5050 - val_loss: 0.7306\n",
      "Epoch 67/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6395 - loss: 0.6394 - val_accuracy: 0.5250 - val_loss: 0.7258\n",
      "Epoch 68/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6273 - loss: 0.6449 - val_accuracy: 0.5050 - val_loss: 0.7324\n",
      "Epoch 69/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6687 - loss: 0.6361 - val_accuracy: 0.5100 - val_loss: 0.7285\n",
      "Epoch 70/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6406 - loss: 0.6419 - val_accuracy: 0.5100 - val_loss: 0.7301\n",
      "Epoch 71/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6461 - loss: 0.6296 - val_accuracy: 0.5250 - val_loss: 0.7290\n",
      "Epoch 72/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6329 - loss: 0.6370 - val_accuracy: 0.5000 - val_loss: 0.7348\n",
      "Epoch 73/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6600 - loss: 0.6308 - val_accuracy: 0.5100 - val_loss: 0.7355\n",
      "Epoch 74/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6150 - loss: 0.6437 - val_accuracy: 0.5050 - val_loss: 0.7334\n",
      "Epoch 75/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6503 - loss: 0.6390 - val_accuracy: 0.5150 - val_loss: 0.7383\n",
      "Epoch 76/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6435 - loss: 0.6320 - val_accuracy: 0.4900 - val_loss: 0.7385\n",
      "Epoch 77/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6268 - loss: 0.6474 - val_accuracy: 0.5100 - val_loss: 0.7330\n",
      "Epoch 78/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6292 - loss: 0.6529 - val_accuracy: 0.5000 - val_loss: 0.7350\n",
      "Epoch 79/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6663 - loss: 0.6230 - val_accuracy: 0.5000 - val_loss: 0.7359\n",
      "Epoch 80/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6607 - loss: 0.6319 - val_accuracy: 0.5000 - val_loss: 0.7361\n",
      "Epoch 81/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6497 - loss: 0.6348 - val_accuracy: 0.5000 - val_loss: 0.7403\n",
      "Epoch 82/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6402 - loss: 0.6338 - val_accuracy: 0.4750 - val_loss: 0.7386\n",
      "Epoch 83/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6114 - loss: 0.6481 - val_accuracy: 0.5000 - val_loss: 0.7388\n",
      "Epoch 84/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6546 - loss: 0.6420 - val_accuracy: 0.5050 - val_loss: 0.7390\n",
      "Epoch 85/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6693 - loss: 0.6166 - val_accuracy: 0.5150 - val_loss: 0.7422\n",
      "Epoch 86/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6659 - loss: 0.6182 - val_accuracy: 0.5350 - val_loss: 0.7361\n",
      "Epoch 87/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6431 - loss: 0.6304 - val_accuracy: 0.5000 - val_loss: 0.7404\n",
      "Epoch 88/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6406 - loss: 0.6328 - val_accuracy: 0.5050 - val_loss: 0.7422\n",
      "Epoch 89/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6335 - loss: 0.6448 - val_accuracy: 0.5150 - val_loss: 0.7386\n",
      "Epoch 90/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6404 - loss: 0.6383 - val_accuracy: 0.5000 - val_loss: 0.7388\n",
      "Epoch 91/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6449 - loss: 0.6457 - val_accuracy: 0.5250 - val_loss: 0.7386\n",
      "Epoch 92/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6515 - loss: 0.6262 - val_accuracy: 0.4900 - val_loss: 0.7410\n",
      "Epoch 93/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6723 - loss: 0.6223 - val_accuracy: 0.5000 - val_loss: 0.7373\n",
      "Epoch 94/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6636 - loss: 0.6266 - val_accuracy: 0.5150 - val_loss: 0.7400\n",
      "Epoch 95/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6253 - loss: 0.6379 - val_accuracy: 0.5200 - val_loss: 0.7394\n",
      "Epoch 96/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6586 - loss: 0.6159 - val_accuracy: 0.5250 - val_loss: 0.7427\n",
      "Epoch 97/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6593 - loss: 0.6312 - val_accuracy: 0.5250 - val_loss: 0.7402\n",
      "Epoch 98/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6749 - loss: 0.6237 - val_accuracy: 0.5100 - val_loss: 0.7383\n",
      "Epoch 99/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6703 - loss: 0.6139 - val_accuracy: 0.5200 - val_loss: 0.7399\n",
      "Epoch 100/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6555 - loss: 0.6207 - val_accuracy: 0.5200 - val_loss: 0.7389\n",
      "Epoch 101/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6687 - loss: 0.6117 - val_accuracy: 0.5050 - val_loss: 0.7383\n",
      "Epoch 102/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6578 - loss: 0.6253 - val_accuracy: 0.5050 - val_loss: 0.7398\n",
      "Epoch 103/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6317 - loss: 0.6424 - val_accuracy: 0.5250 - val_loss: 0.7344\n",
      "Epoch 104/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6515 - loss: 0.6234 - val_accuracy: 0.5150 - val_loss: 0.7395\n",
      "Epoch 105/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6280 - loss: 0.6434 - val_accuracy: 0.5150 - val_loss: 0.7402\n",
      "Epoch 106/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6378 - loss: 0.6328 - val_accuracy: 0.5100 - val_loss: 0.7398\n",
      "Epoch 107/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6489 - loss: 0.6249 - val_accuracy: 0.5100 - val_loss: 0.7377\n",
      "Epoch 108/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6839 - loss: 0.6064 - val_accuracy: 0.5100 - val_loss: 0.7376\n",
      "Epoch 109/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6778 - loss: 0.6190 - val_accuracy: 0.5000 - val_loss: 0.7394\n",
      "Epoch 110/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6647 - loss: 0.6197 - val_accuracy: 0.5350 - val_loss: 0.7392\n",
      "Epoch 111/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6861 - loss: 0.6173 - val_accuracy: 0.5150 - val_loss: 0.7348\n",
      "Epoch 112/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6680 - loss: 0.6262 - val_accuracy: 0.5400 - val_loss: 0.7377\n",
      "Epoch 113/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6567 - loss: 0.6235 - val_accuracy: 0.5150 - val_loss: 0.7357\n",
      "Epoch 114/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6459 - loss: 0.6186 - val_accuracy: 0.5150 - val_loss: 0.7348\n",
      "Epoch 115/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6256 - loss: 0.6377 - val_accuracy: 0.5200 - val_loss: 0.7386\n",
      "Epoch 116/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6687 - loss: 0.6229 - val_accuracy: 0.5250 - val_loss: 0.7373\n",
      "Epoch 117/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6598 - loss: 0.6253 - val_accuracy: 0.5150 - val_loss: 0.7483\n",
      "Epoch 118/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6642 - loss: 0.6242 - val_accuracy: 0.5100 - val_loss: 0.7469\n",
      "Epoch 119/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6794 - loss: 0.6159 - val_accuracy: 0.5200 - val_loss: 0.7423\n",
      "Epoch 120/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6661 - loss: 0.6302 - val_accuracy: 0.5200 - val_loss: 0.7402\n",
      "Epoch 121/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6468 - loss: 0.6198 - val_accuracy: 0.4950 - val_loss: 0.7420\n",
      "Epoch 122/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6411 - loss: 0.6417 - val_accuracy: 0.5000 - val_loss: 0.7371\n",
      "Epoch 123/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6441 - loss: 0.6264 - val_accuracy: 0.5250 - val_loss: 0.7387\n",
      "Epoch 124/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6716 - loss: 0.6116 - val_accuracy: 0.5150 - val_loss: 0.7370\n",
      "Epoch 125/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6689 - loss: 0.6079 - val_accuracy: 0.5050 - val_loss: 0.7409\n",
      "Epoch 126/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6543 - loss: 0.6185 - val_accuracy: 0.5000 - val_loss: 0.7426\n",
      "Epoch 127/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6296 - loss: 0.6361 - val_accuracy: 0.5150 - val_loss: 0.7361\n",
      "Epoch 128/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6794 - loss: 0.6143 - val_accuracy: 0.5250 - val_loss: 0.7385\n",
      "Epoch 129/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6476 - loss: 0.6203 - val_accuracy: 0.5250 - val_loss: 0.7403\n",
      "Epoch 130/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6491 - loss: 0.6279 - val_accuracy: 0.5050 - val_loss: 0.7399\n",
      "Epoch 131/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6922 - loss: 0.6024 - val_accuracy: 0.5150 - val_loss: 0.7423\n",
      "Epoch 132/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6605 - loss: 0.6179 - val_accuracy: 0.5150 - val_loss: 0.7343\n",
      "Epoch 133/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6577 - loss: 0.6262 - val_accuracy: 0.5100 - val_loss: 0.7371\n",
      "Epoch 134/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6657 - loss: 0.6092 - val_accuracy: 0.5300 - val_loss: 0.7430\n",
      "Epoch 135/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6448 - loss: 0.6315 - val_accuracy: 0.5050 - val_loss: 0.7427\n",
      "Epoch 136/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6803 - loss: 0.6048 - val_accuracy: 0.5150 - val_loss: 0.7414\n",
      "Epoch 137/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6605 - loss: 0.6186 - val_accuracy: 0.5000 - val_loss: 0.7418\n",
      "Epoch 138/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6612 - loss: 0.6201 - val_accuracy: 0.5150 - val_loss: 0.7412\n",
      "Epoch 139/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6767 - loss: 0.6188 - val_accuracy: 0.5150 - val_loss: 0.7372\n",
      "Epoch 140/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6933 - loss: 0.6061 - val_accuracy: 0.5250 - val_loss: 0.7421\n",
      "Epoch 141/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6640 - loss: 0.6192 - val_accuracy: 0.5100 - val_loss: 0.7398\n",
      "Epoch 142/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6767 - loss: 0.6177 - val_accuracy: 0.5150 - val_loss: 0.7391\n",
      "Epoch 143/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6507 - loss: 0.6135 - val_accuracy: 0.5350 - val_loss: 0.7392\n",
      "Epoch 144/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6367 - loss: 0.6323 - val_accuracy: 0.5150 - val_loss: 0.7423\n",
      "Epoch 145/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6343 - loss: 0.6319 - val_accuracy: 0.5300 - val_loss: 0.7382\n",
      "Epoch 146/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6500 - loss: 0.6164 - val_accuracy: 0.5200 - val_loss: 0.7395\n",
      "Epoch 147/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6632 - loss: 0.6104 - val_accuracy: 0.5150 - val_loss: 0.7476\n",
      "Epoch 148/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6681 - loss: 0.6118 - val_accuracy: 0.5100 - val_loss: 0.7422\n",
      "Epoch 149/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6442 - loss: 0.6256 - val_accuracy: 0.5250 - val_loss: 0.7369\n",
      "Epoch 150/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6357 - loss: 0.6386 - val_accuracy: 0.5200 - val_loss: 0.7387\n",
      "Epoch 151/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6518 - loss: 0.6245 - val_accuracy: 0.5050 - val_loss: 0.7416\n",
      "Epoch 152/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6684 - loss: 0.6206 - val_accuracy: 0.5050 - val_loss: 0.7428\n",
      "Epoch 153/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6746 - loss: 0.6056 - val_accuracy: 0.5050 - val_loss: 0.7466\n",
      "Epoch 154/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6822 - loss: 0.6025 - val_accuracy: 0.4900 - val_loss: 0.7469\n",
      "Epoch 155/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6493 - loss: 0.6170 - val_accuracy: 0.5200 - val_loss: 0.7426\n",
      "Epoch 156/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6312 - loss: 0.6232 - val_accuracy: 0.4950 - val_loss: 0.7457\n",
      "Epoch 157/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6673 - loss: 0.6082 - val_accuracy: 0.5350 - val_loss: 0.7433\n",
      "Epoch 158/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6578 - loss: 0.6227 - val_accuracy: 0.5200 - val_loss: 0.7396\n",
      "Epoch 159/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6838 - loss: 0.6096 - val_accuracy: 0.5150 - val_loss: 0.7456\n",
      "Epoch 160/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6773 - loss: 0.6060 - val_accuracy: 0.5200 - val_loss: 0.7455\n",
      "Epoch 161/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6722 - loss: 0.6061 - val_accuracy: 0.5300 - val_loss: 0.7432\n",
      "Epoch 162/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6697 - loss: 0.6109 - val_accuracy: 0.5100 - val_loss: 0.7454\n",
      "Epoch 163/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6811 - loss: 0.6013 - val_accuracy: 0.4950 - val_loss: 0.7484\n",
      "Epoch 164/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6642 - loss: 0.6145 - val_accuracy: 0.5000 - val_loss: 0.7470\n",
      "Epoch 165/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6452 - loss: 0.6236 - val_accuracy: 0.5150 - val_loss: 0.7454\n",
      "Epoch 166/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6443 - loss: 0.6226 - val_accuracy: 0.5150 - val_loss: 0.7457\n",
      "Epoch 167/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6643 - loss: 0.6187 - val_accuracy: 0.5150 - val_loss: 0.7462\n",
      "Epoch 168/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6504 - loss: 0.6197 - val_accuracy: 0.5200 - val_loss: 0.7417\n",
      "Epoch 169/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6591 - loss: 0.6207 - val_accuracy: 0.5100 - val_loss: 0.7480\n",
      "Epoch 170/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6772 - loss: 0.6155 - val_accuracy: 0.5200 - val_loss: 0.7470\n",
      "Epoch 171/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6720 - loss: 0.6102 - val_accuracy: 0.5100 - val_loss: 0.7505\n",
      "Epoch 172/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6975 - loss: 0.5969 - val_accuracy: 0.5050 - val_loss: 0.7469\n",
      "Epoch 173/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6841 - loss: 0.6059 - val_accuracy: 0.5150 - val_loss: 0.7464\n",
      "Epoch 174/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6959 - loss: 0.6058 - val_accuracy: 0.5250 - val_loss: 0.7497\n",
      "Epoch 175/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6698 - loss: 0.6141 - val_accuracy: 0.5150 - val_loss: 0.7497\n",
      "Epoch 176/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6520 - loss: 0.6242 - val_accuracy: 0.5000 - val_loss: 0.7457\n",
      "Epoch 177/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6766 - loss: 0.5994 - val_accuracy: 0.5000 - val_loss: 0.7472\n",
      "Epoch 178/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6575 - loss: 0.6146 - val_accuracy: 0.5200 - val_loss: 0.7477\n",
      "Epoch 179/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6722 - loss: 0.6124 - val_accuracy: 0.4950 - val_loss: 0.7535\n",
      "Epoch 180/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6528 - loss: 0.6259 - val_accuracy: 0.5000 - val_loss: 0.7432\n",
      "Epoch 181/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6482 - loss: 0.6197 - val_accuracy: 0.4950 - val_loss: 0.7525\n",
      "Epoch 182/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6685 - loss: 0.6040 - val_accuracy: 0.5150 - val_loss: 0.7420\n",
      "Epoch 183/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6802 - loss: 0.5995 - val_accuracy: 0.5050 - val_loss: 0.7479\n",
      "Epoch 184/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6313 - loss: 0.6245 - val_accuracy: 0.5100 - val_loss: 0.7542\n",
      "Epoch 185/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6818 - loss: 0.5985 - val_accuracy: 0.5000 - val_loss: 0.7530\n",
      "Epoch 186/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6693 - loss: 0.6213 - val_accuracy: 0.5050 - val_loss: 0.7524\n",
      "Epoch 187/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6738 - loss: 0.6004 - val_accuracy: 0.5050 - val_loss: 0.7563\n",
      "Epoch 188/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6395 - loss: 0.6439 - val_accuracy: 0.5000 - val_loss: 0.7549\n",
      "Epoch 189/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6815 - loss: 0.5971 - val_accuracy: 0.5100 - val_loss: 0.7481\n",
      "Epoch 190/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6723 - loss: 0.6036 - val_accuracy: 0.5050 - val_loss: 0.7530\n",
      "Epoch 191/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6728 - loss: 0.6036 - val_accuracy: 0.5000 - val_loss: 0.7525\n",
      "Epoch 192/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6751 - loss: 0.6080 - val_accuracy: 0.5100 - val_loss: 0.7456\n",
      "Epoch 193/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6819 - loss: 0.5950 - val_accuracy: 0.5050 - val_loss: 0.7458\n",
      "Epoch 194/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6842 - loss: 0.6017 - val_accuracy: 0.5050 - val_loss: 0.7456\n",
      "Epoch 195/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6885 - loss: 0.5991 - val_accuracy: 0.5150 - val_loss: 0.7411\n",
      "Epoch 196/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6693 - loss: 0.6139 - val_accuracy: 0.5100 - val_loss: 0.7401\n",
      "Epoch 197/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6886 - loss: 0.5913 - val_accuracy: 0.5150 - val_loss: 0.7457\n",
      "Epoch 198/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6768 - loss: 0.6056 - val_accuracy: 0.5050 - val_loss: 0.7450\n",
      "Epoch 199/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6674 - loss: 0.6064 - val_accuracy: 0.5150 - val_loss: 0.7401\n",
      "Epoch 200/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6747 - loss: 0.6082 - val_accuracy: 0.5200 - val_loss: 0.7500\n",
      "Epoch 201/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6568 - loss: 0.6129 - val_accuracy: 0.5050 - val_loss: 0.7474\n",
      "Epoch 202/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6681 - loss: 0.5927 - val_accuracy: 0.5100 - val_loss: 0.7468\n",
      "Epoch 203/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6832 - loss: 0.6119 - val_accuracy: 0.5050 - val_loss: 0.7472\n",
      "Epoch 204/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6801 - loss: 0.5993 - val_accuracy: 0.5000 - val_loss: 0.7451\n",
      "Epoch 205/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6840 - loss: 0.6046 - val_accuracy: 0.5150 - val_loss: 0.7474\n",
      "Epoch 206/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6615 - loss: 0.6071 - val_accuracy: 0.4950 - val_loss: 0.7478\n",
      "Epoch 207/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6999 - loss: 0.6103 - val_accuracy: 0.4900 - val_loss: 0.7460\n",
      "Epoch 208/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6782 - loss: 0.6119 - val_accuracy: 0.5050 - val_loss: 0.7464\n",
      "Epoch 209/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6663 - loss: 0.6079 - val_accuracy: 0.5300 - val_loss: 0.7478\n",
      "Epoch 210/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6735 - loss: 0.6070 - val_accuracy: 0.5300 - val_loss: 0.7495\n",
      "Epoch 211/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6966 - loss: 0.5834 - val_accuracy: 0.5150 - val_loss: 0.7498\n",
      "Epoch 212/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6440 - loss: 0.6176 - val_accuracy: 0.5200 - val_loss: 0.7484\n",
      "Epoch 213/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6754 - loss: 0.6030 - val_accuracy: 0.5250 - val_loss: 0.7494\n",
      "Epoch 214/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6780 - loss: 0.5901 - val_accuracy: 0.5350 - val_loss: 0.7455\n",
      "Epoch 215/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6779 - loss: 0.6075 - val_accuracy: 0.5200 - val_loss: 0.7479\n",
      "Epoch 216/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6926 - loss: 0.5984 - val_accuracy: 0.5250 - val_loss: 0.7539\n",
      "Epoch 217/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6548 - loss: 0.6179 - val_accuracy: 0.5150 - val_loss: 0.7504\n",
      "Epoch 218/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6795 - loss: 0.5893 - val_accuracy: 0.5050 - val_loss: 0.7459\n",
      "Epoch 219/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6775 - loss: 0.6021 - val_accuracy: 0.5150 - val_loss: 0.7486\n",
      "Epoch 220/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6529 - loss: 0.6079 - val_accuracy: 0.5050 - val_loss: 0.7499\n",
      "Epoch 221/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6783 - loss: 0.6013 - val_accuracy: 0.5200 - val_loss: 0.7464\n",
      "Epoch 222/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6794 - loss: 0.6073 - val_accuracy: 0.5100 - val_loss: 0.7521\n",
      "Epoch 223/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6659 - loss: 0.6078 - val_accuracy: 0.5300 - val_loss: 0.7365\n",
      "Epoch 224/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6818 - loss: 0.6029 - val_accuracy: 0.5150 - val_loss: 0.7484\n",
      "Epoch 225/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6719 - loss: 0.6069 - val_accuracy: 0.5000 - val_loss: 0.7493\n",
      "Epoch 226/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6563 - loss: 0.6045 - val_accuracy: 0.5050 - val_loss: 0.7465\n",
      "Epoch 227/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6659 - loss: 0.6260 - val_accuracy: 0.5050 - val_loss: 0.7512\n",
      "Epoch 228/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6825 - loss: 0.6051 - val_accuracy: 0.5100 - val_loss: 0.7479\n",
      "Epoch 229/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6773 - loss: 0.6001 - val_accuracy: 0.5150 - val_loss: 0.7489\n",
      "Epoch 230/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6851 - loss: 0.5919 - val_accuracy: 0.5300 - val_loss: 0.7512\n",
      "Epoch 231/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6753 - loss: 0.5910 - val_accuracy: 0.5200 - val_loss: 0.7432\n",
      "Epoch 232/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6986 - loss: 0.5800 - val_accuracy: 0.5100 - val_loss: 0.7502\n",
      "Epoch 233/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6673 - loss: 0.6037 - val_accuracy: 0.5150 - val_loss: 0.7442\n",
      "Epoch 234/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6748 - loss: 0.6124 - val_accuracy: 0.5250 - val_loss: 0.7490\n",
      "Epoch 235/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6834 - loss: 0.5976 - val_accuracy: 0.5200 - val_loss: 0.7479\n",
      "Epoch 236/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6941 - loss: 0.5876 - val_accuracy: 0.5250 - val_loss: 0.7520\n",
      "Epoch 237/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6696 - loss: 0.6127 - val_accuracy: 0.5200 - val_loss: 0.7529\n",
      "Epoch 238/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7089 - loss: 0.5782 - val_accuracy: 0.5250 - val_loss: 0.7483\n",
      "Epoch 239/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6791 - loss: 0.6066 - val_accuracy: 0.5000 - val_loss: 0.7507\n",
      "Epoch 240/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6712 - loss: 0.6203 - val_accuracy: 0.5100 - val_loss: 0.7516\n",
      "Epoch 241/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6684 - loss: 0.6074 - val_accuracy: 0.4950 - val_loss: 0.7586\n",
      "Epoch 242/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6876 - loss: 0.6015 - val_accuracy: 0.5050 - val_loss: 0.7536\n",
      "Epoch 243/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7019 - loss: 0.5806 - val_accuracy: 0.5300 - val_loss: 0.7506\n",
      "Epoch 244/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6758 - loss: 0.6018 - val_accuracy: 0.5100 - val_loss: 0.7565\n",
      "Epoch 245/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6958 - loss: 0.5777 - val_accuracy: 0.5050 - val_loss: 0.7490\n",
      "Epoch 246/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6408 - loss: 0.6379 - val_accuracy: 0.5200 - val_loss: 0.7510\n",
      "Epoch 247/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7061 - loss: 0.5831 - val_accuracy: 0.5150 - val_loss: 0.7539\n",
      "Epoch 248/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6761 - loss: 0.5936 - val_accuracy: 0.5050 - val_loss: 0.7512\n",
      "Epoch 249/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6783 - loss: 0.6026 - val_accuracy: 0.5150 - val_loss: 0.7555\n",
      "Epoch 250/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6674 - loss: 0.6175 - val_accuracy: 0.5050 - val_loss: 0.7541\n",
      "Epoch 251/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6864 - loss: 0.6050 - val_accuracy: 0.5200 - val_loss: 0.7540\n",
      "Epoch 252/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6678 - loss: 0.6130 - val_accuracy: 0.5250 - val_loss: 0.7565\n",
      "Epoch 253/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6643 - loss: 0.6034 - val_accuracy: 0.5400 - val_loss: 0.7491\n",
      "Epoch 254/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6783 - loss: 0.6054 - val_accuracy: 0.5200 - val_loss: 0.7511\n",
      "Epoch 255/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6949 - loss: 0.5779 - val_accuracy: 0.5050 - val_loss: 0.7544\n",
      "Epoch 256/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6890 - loss: 0.5897 - val_accuracy: 0.5100 - val_loss: 0.7526\n",
      "Epoch 257/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6736 - loss: 0.6169 - val_accuracy: 0.5150 - val_loss: 0.7548\n",
      "Epoch 258/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7019 - loss: 0.5793 - val_accuracy: 0.5100 - val_loss: 0.7511\n",
      "Epoch 259/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6676 - loss: 0.6073 - val_accuracy: 0.5050 - val_loss: 0.7523\n",
      "Epoch 260/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6631 - loss: 0.6149 - val_accuracy: 0.5200 - val_loss: 0.7552\n",
      "Epoch 261/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6696 - loss: 0.6104 - val_accuracy: 0.5050 - val_loss: 0.7505\n",
      "Epoch 262/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7052 - loss: 0.5920 - val_accuracy: 0.5000 - val_loss: 0.7480\n",
      "Epoch 263/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6839 - loss: 0.5964 - val_accuracy: 0.5000 - val_loss: 0.7485\n",
      "Epoch 264/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6915 - loss: 0.5994 - val_accuracy: 0.5150 - val_loss: 0.7492\n",
      "Epoch 265/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6810 - loss: 0.6116 - val_accuracy: 0.5100 - val_loss: 0.7512\n",
      "Epoch 266/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6962 - loss: 0.5935 - val_accuracy: 0.5100 - val_loss: 0.7499\n",
      "Epoch 267/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6926 - loss: 0.5863 - val_accuracy: 0.5000 - val_loss: 0.7535\n",
      "Epoch 268/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6917 - loss: 0.5920 - val_accuracy: 0.4950 - val_loss: 0.7557\n",
      "Epoch 269/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6973 - loss: 0.5902 - val_accuracy: 0.4850 - val_loss: 0.7560\n",
      "Epoch 270/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6787 - loss: 0.5928 - val_accuracy: 0.5000 - val_loss: 0.7529\n",
      "Epoch 271/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6998 - loss: 0.5988 - val_accuracy: 0.5100 - val_loss: 0.7542\n",
      "Epoch 272/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6690 - loss: 0.5965 - val_accuracy: 0.5200 - val_loss: 0.7560\n",
      "Epoch 273/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7016 - loss: 0.5782 - val_accuracy: 0.5250 - val_loss: 0.7581\n",
      "Epoch 274/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6942 - loss: 0.5815 - val_accuracy: 0.5300 - val_loss: 0.7551\n",
      "Epoch 275/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6871 - loss: 0.5862 - val_accuracy: 0.5200 - val_loss: 0.7573\n",
      "Epoch 276/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6690 - loss: 0.5927 - val_accuracy: 0.5250 - val_loss: 0.7551\n",
      "Epoch 277/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6678 - loss: 0.5954 - val_accuracy: 0.5250 - val_loss: 0.7572\n",
      "Epoch 278/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6871 - loss: 0.5810 - val_accuracy: 0.5150 - val_loss: 0.7634\n",
      "Epoch 279/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6548 - loss: 0.6010 - val_accuracy: 0.5200 - val_loss: 0.7617\n",
      "Epoch 280/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6723 - loss: 0.5943 - val_accuracy: 0.5150 - val_loss: 0.7575\n",
      "Epoch 281/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6926 - loss: 0.5891 - val_accuracy: 0.5200 - val_loss: 0.7622\n",
      "Epoch 282/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6618 - loss: 0.6130 - val_accuracy: 0.5300 - val_loss: 0.7557\n",
      "Epoch 283/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6782 - loss: 0.6043 - val_accuracy: 0.5200 - val_loss: 0.7617\n",
      "Epoch 284/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6923 - loss: 0.5811 - val_accuracy: 0.5000 - val_loss: 0.7646\n",
      "Epoch 285/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6746 - loss: 0.5997 - val_accuracy: 0.5300 - val_loss: 0.7555\n",
      "Epoch 286/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6865 - loss: 0.5967 - val_accuracy: 0.5100 - val_loss: 0.7642\n",
      "Epoch 287/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6911 - loss: 0.5789 - val_accuracy: 0.5100 - val_loss: 0.7686\n",
      "Epoch 288/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6974 - loss: 0.5887 - val_accuracy: 0.5100 - val_loss: 0.7633\n",
      "Epoch 289/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6956 - loss: 0.5760 - val_accuracy: 0.5150 - val_loss: 0.7631\n",
      "Epoch 290/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6725 - loss: 0.5967 - val_accuracy: 0.5300 - val_loss: 0.7604\n",
      "Epoch 291/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6717 - loss: 0.5866 - val_accuracy: 0.5000 - val_loss: 0.7622\n",
      "Epoch 292/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6756 - loss: 0.6087 - val_accuracy: 0.5000 - val_loss: 0.7634\n",
      "Epoch 293/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7256 - loss: 0.5665 - val_accuracy: 0.5150 - val_loss: 0.7720\n",
      "Epoch 294/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7139 - loss: 0.5770 - val_accuracy: 0.5050 - val_loss: 0.7649\n",
      "Epoch 295/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6799 - loss: 0.5892 - val_accuracy: 0.5100 - val_loss: 0.7591\n",
      "Epoch 296/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6875 - loss: 0.5917 - val_accuracy: 0.5050 - val_loss: 0.7693\n",
      "Epoch 297/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6906 - loss: 0.5951 - val_accuracy: 0.5250 - val_loss: 0.7587\n",
      "Epoch 298/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6876 - loss: 0.5922 - val_accuracy: 0.5200 - val_loss: 0.7639\n",
      "Epoch 299/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6852 - loss: 0.5913 - val_accuracy: 0.5050 - val_loss: 0.7625\n",
      "Epoch 300/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7162 - loss: 0.5808 - val_accuracy: 0.4950 - val_loss: 0.7647\n",
      "Epoch 301/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7031 - loss: 0.5795 - val_accuracy: 0.5100 - val_loss: 0.7615\n",
      "Epoch 302/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6678 - loss: 0.5934 - val_accuracy: 0.5300 - val_loss: 0.7600\n",
      "Epoch 303/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6951 - loss: 0.5816 - val_accuracy: 0.4900 - val_loss: 0.7765\n",
      "Epoch 304/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6617 - loss: 0.5940 - val_accuracy: 0.5250 - val_loss: 0.7687\n",
      "Epoch 305/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6824 - loss: 0.5943 - val_accuracy: 0.5200 - val_loss: 0.7636\n",
      "Epoch 306/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6734 - loss: 0.6059 - val_accuracy: 0.5150 - val_loss: 0.7713\n",
      "Epoch 307/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7043 - loss: 0.5644 - val_accuracy: 0.5300 - val_loss: 0.7597\n",
      "Epoch 308/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6830 - loss: 0.5826 - val_accuracy: 0.5400 - val_loss: 0.7565\n",
      "Epoch 309/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6979 - loss: 0.5886 - val_accuracy: 0.5150 - val_loss: 0.7635\n",
      "Epoch 310/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7230 - loss: 0.5650 - val_accuracy: 0.5150 - val_loss: 0.7591\n",
      "Epoch 311/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6727 - loss: 0.5977 - val_accuracy: 0.5250 - val_loss: 0.7608\n",
      "Epoch 312/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6759 - loss: 0.5954 - val_accuracy: 0.5200 - val_loss: 0.7642\n",
      "Epoch 313/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6895 - loss: 0.5852 - val_accuracy: 0.5300 - val_loss: 0.7677\n",
      "Epoch 314/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6832 - loss: 0.5958 - val_accuracy: 0.5250 - val_loss: 0.7661\n",
      "Epoch 315/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7006 - loss: 0.5822 - val_accuracy: 0.5100 - val_loss: 0.7698\n",
      "Epoch 316/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7052 - loss: 0.5748 - val_accuracy: 0.5100 - val_loss: 0.7720\n",
      "Epoch 317/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6509 - loss: 0.6216 - val_accuracy: 0.5350 - val_loss: 0.7648\n",
      "Epoch 318/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6766 - loss: 0.5904 - val_accuracy: 0.5200 - val_loss: 0.7652\n",
      "Epoch 319/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6903 - loss: 0.5718 - val_accuracy: 0.5150 - val_loss: 0.7714\n",
      "Epoch 320/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6877 - loss: 0.5812 - val_accuracy: 0.5350 - val_loss: 0.7624\n",
      "Epoch 321/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6816 - loss: 0.5834 - val_accuracy: 0.5300 - val_loss: 0.7661\n",
      "Epoch 322/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7003 - loss: 0.5725 - val_accuracy: 0.5100 - val_loss: 0.7737\n",
      "Epoch 323/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7037 - loss: 0.5826 - val_accuracy: 0.5300 - val_loss: 0.7693\n",
      "Epoch 324/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7022 - loss: 0.5682 - val_accuracy: 0.5200 - val_loss: 0.7670\n",
      "Epoch 325/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7107 - loss: 0.5646 - val_accuracy: 0.5350 - val_loss: 0.7673\n",
      "Epoch 326/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6826 - loss: 0.5963 - val_accuracy: 0.5300 - val_loss: 0.7673\n",
      "Epoch 327/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6823 - loss: 0.5969 - val_accuracy: 0.5350 - val_loss: 0.7721\n",
      "Epoch 328/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6930 - loss: 0.5843 - val_accuracy: 0.5150 - val_loss: 0.7747\n",
      "Epoch 329/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6624 - loss: 0.6014 - val_accuracy: 0.5200 - val_loss: 0.7782\n",
      "Epoch 330/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7122 - loss: 0.5833 - val_accuracy: 0.5300 - val_loss: 0.7750\n",
      "Epoch 331/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6843 - loss: 0.5839 - val_accuracy: 0.5200 - val_loss: 0.7813\n",
      "Epoch 332/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6718 - loss: 0.6071 - val_accuracy: 0.5250 - val_loss: 0.7746\n",
      "Epoch 333/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7214 - loss: 0.5657 - val_accuracy: 0.5350 - val_loss: 0.7695\n",
      "Epoch 334/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7002 - loss: 0.5819 - val_accuracy: 0.5400 - val_loss: 0.7721\n",
      "Epoch 335/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6892 - loss: 0.5799 - val_accuracy: 0.5250 - val_loss: 0.7716\n",
      "Epoch 336/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6957 - loss: 0.5878 - val_accuracy: 0.5150 - val_loss: 0.7764\n",
      "Epoch 337/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6923 - loss: 0.5765 - val_accuracy: 0.5350 - val_loss: 0.7716\n",
      "Epoch 338/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6814 - loss: 0.5933 - val_accuracy: 0.5400 - val_loss: 0.7676\n",
      "Epoch 339/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6688 - loss: 0.6083 - val_accuracy: 0.5250 - val_loss: 0.7693\n",
      "Epoch 340/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7061 - loss: 0.5705 - val_accuracy: 0.5100 - val_loss: 0.7781\n",
      "Epoch 341/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6970 - loss: 0.5656 - val_accuracy: 0.5250 - val_loss: 0.7747\n",
      "Epoch 342/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7068 - loss: 0.5706 - val_accuracy: 0.5200 - val_loss: 0.7720\n",
      "Epoch 343/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6920 - loss: 0.5690 - val_accuracy: 0.5450 - val_loss: 0.7739\n",
      "Epoch 344/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6972 - loss: 0.5841 - val_accuracy: 0.5300 - val_loss: 0.7763\n",
      "Epoch 345/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6924 - loss: 0.5891 - val_accuracy: 0.5350 - val_loss: 0.7750\n",
      "Epoch 346/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6997 - loss: 0.5884 - val_accuracy: 0.5350 - val_loss: 0.7795\n",
      "Epoch 347/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6960 - loss: 0.5768 - val_accuracy: 0.5250 - val_loss: 0.7852\n",
      "Epoch 348/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7159 - loss: 0.5637 - val_accuracy: 0.5150 - val_loss: 0.7863\n",
      "Epoch 349/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6866 - loss: 0.5824 - val_accuracy: 0.5300 - val_loss: 0.7898\n",
      "Epoch 350/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6647 - loss: 0.5869 - val_accuracy: 0.5300 - val_loss: 0.7837\n",
      "Epoch 351/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6713 - loss: 0.5936 - val_accuracy: 0.5200 - val_loss: 0.7818\n",
      "Epoch 352/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6715 - loss: 0.5872 - val_accuracy: 0.5200 - val_loss: 0.7844\n",
      "Epoch 353/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7196 - loss: 0.5555 - val_accuracy: 0.5300 - val_loss: 0.7872\n",
      "Epoch 354/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6803 - loss: 0.5948 - val_accuracy: 0.5400 - val_loss: 0.7794\n",
      "Epoch 355/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6919 - loss: 0.5727 - val_accuracy: 0.5300 - val_loss: 0.7834\n",
      "Epoch 356/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6801 - loss: 0.5870 - val_accuracy: 0.5150 - val_loss: 0.7897\n",
      "Epoch 357/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7092 - loss: 0.5655 - val_accuracy: 0.5250 - val_loss: 0.7894\n",
      "Epoch 358/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6932 - loss: 0.5785 - val_accuracy: 0.5300 - val_loss: 0.7852\n",
      "Epoch 359/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6597 - loss: 0.6112 - val_accuracy: 0.5350 - val_loss: 0.7864\n",
      "Epoch 360/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6708 - loss: 0.5878 - val_accuracy: 0.5400 - val_loss: 0.7761\n",
      "Epoch 361/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7063 - loss: 0.5683 - val_accuracy: 0.5150 - val_loss: 0.7777\n",
      "Epoch 362/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7048 - loss: 0.5668 - val_accuracy: 0.5300 - val_loss: 0.7768\n",
      "Epoch 363/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6783 - loss: 0.5791 - val_accuracy: 0.5100 - val_loss: 0.7794\n",
      "Epoch 364/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7040 - loss: 0.5783 - val_accuracy: 0.5350 - val_loss: 0.7830\n",
      "Epoch 365/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7269 - loss: 0.5538 - val_accuracy: 0.5400 - val_loss: 0.7774\n",
      "Epoch 366/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6978 - loss: 0.5835 - val_accuracy: 0.5400 - val_loss: 0.7824\n",
      "Epoch 367/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6943 - loss: 0.5764 - val_accuracy: 0.5350 - val_loss: 0.7831\n",
      "Epoch 368/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6888 - loss: 0.5778 - val_accuracy: 0.5450 - val_loss: 0.7816\n",
      "Epoch 369/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6883 - loss: 0.5623 - val_accuracy: 0.5250 - val_loss: 0.7919\n",
      "Epoch 370/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7120 - loss: 0.5627 - val_accuracy: 0.5400 - val_loss: 0.7811\n",
      "Epoch 371/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7163 - loss: 0.5631 - val_accuracy: 0.5450 - val_loss: 0.7813\n",
      "Epoch 372/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7260 - loss: 0.5585 - val_accuracy: 0.5300 - val_loss: 0.7951\n",
      "Epoch 373/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7152 - loss: 0.5573 - val_accuracy: 0.5350 - val_loss: 0.7874\n",
      "Epoch 374/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6444 - loss: 0.6119 - val_accuracy: 0.5350 - val_loss: 0.7877\n",
      "Epoch 375/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7323 - loss: 0.5479 - val_accuracy: 0.5300 - val_loss: 0.7966\n",
      "Epoch 376/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6950 - loss: 0.5861 - val_accuracy: 0.5450 - val_loss: 0.7883\n",
      "Epoch 377/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6813 - loss: 0.5979 - val_accuracy: 0.5400 - val_loss: 0.7874\n",
      "Epoch 378/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6659 - loss: 0.5820 - val_accuracy: 0.5300 - val_loss: 0.7877\n",
      "Epoch 379/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6714 - loss: 0.5848 - val_accuracy: 0.5200 - val_loss: 0.7899\n",
      "Epoch 380/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7093 - loss: 0.5731 - val_accuracy: 0.5300 - val_loss: 0.7946\n",
      "Epoch 381/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6895 - loss: 0.5875 - val_accuracy: 0.5300 - val_loss: 0.7893\n",
      "Epoch 382/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7239 - loss: 0.5525 - val_accuracy: 0.5350 - val_loss: 0.7940\n",
      "Epoch 383/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7156 - loss: 0.5691 - val_accuracy: 0.5250 - val_loss: 0.7983\n",
      "Epoch 384/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6999 - loss: 0.5784 - val_accuracy: 0.5350 - val_loss: 0.7964\n",
      "Epoch 385/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7014 - loss: 0.5752 - val_accuracy: 0.5400 - val_loss: 0.7919\n",
      "Epoch 386/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6984 - loss: 0.5777 - val_accuracy: 0.5500 - val_loss: 0.7935\n",
      "Epoch 387/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6779 - loss: 0.5755 - val_accuracy: 0.5200 - val_loss: 0.7993\n",
      "Epoch 388/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6999 - loss: 0.5776 - val_accuracy: 0.5300 - val_loss: 0.8007\n",
      "Epoch 389/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7068 - loss: 0.5822 - val_accuracy: 0.5400 - val_loss: 0.7950\n",
      "Epoch 390/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7015 - loss: 0.5644 - val_accuracy: 0.5350 - val_loss: 0.7963\n",
      "Epoch 391/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6838 - loss: 0.5806 - val_accuracy: 0.5150 - val_loss: 0.8021\n",
      "Epoch 392/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7002 - loss: 0.5678 - val_accuracy: 0.5250 - val_loss: 0.7982\n",
      "Epoch 393/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7292 - loss: 0.5508 - val_accuracy: 0.5300 - val_loss: 0.7975\n",
      "Epoch 394/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6686 - loss: 0.5938 - val_accuracy: 0.5400 - val_loss: 0.8013\n",
      "Epoch 395/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6953 - loss: 0.5730 - val_accuracy: 0.5500 - val_loss: 0.7922\n",
      "Epoch 396/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6827 - loss: 0.5898 - val_accuracy: 0.5400 - val_loss: 0.8015\n",
      "Epoch 397/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7003 - loss: 0.5671 - val_accuracy: 0.5150 - val_loss: 0.7956\n",
      "Epoch 398/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6961 - loss: 0.5625 - val_accuracy: 0.5250 - val_loss: 0.8042\n",
      "Epoch 399/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7073 - loss: 0.5546 - val_accuracy: 0.5450 - val_loss: 0.7994\n",
      "Epoch 400/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6752 - loss: 0.5849 - val_accuracy: 0.5450 - val_loss: 0.8021\n",
      "Epoch 401/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6780 - loss: 0.5882 - val_accuracy: 0.5250 - val_loss: 0.8038\n",
      "Epoch 402/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6991 - loss: 0.5711 - val_accuracy: 0.5150 - val_loss: 0.8031\n",
      "Epoch 403/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7061 - loss: 0.5623 - val_accuracy: 0.5300 - val_loss: 0.7989\n",
      "Epoch 404/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7228 - loss: 0.5537 - val_accuracy: 0.5400 - val_loss: 0.7975\n",
      "Epoch 405/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6972 - loss: 0.5797 - val_accuracy: 0.5350 - val_loss: 0.8060\n",
      "Epoch 406/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7058 - loss: 0.5569 - val_accuracy: 0.5250 - val_loss: 0.8013\n",
      "Epoch 407/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6886 - loss: 0.5617 - val_accuracy: 0.5400 - val_loss: 0.8117\n",
      "Epoch 408/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6924 - loss: 0.5817 - val_accuracy: 0.5400 - val_loss: 0.8176\n",
      "Epoch 409/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6804 - loss: 0.5912 - val_accuracy: 0.5250 - val_loss: 0.8097\n",
      "Epoch 410/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7036 - loss: 0.5588 - val_accuracy: 0.5300 - val_loss: 0.8110\n",
      "Epoch 411/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7266 - loss: 0.5605 - val_accuracy: 0.5150 - val_loss: 0.8149\n",
      "Epoch 412/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7019 - loss: 0.5729 - val_accuracy: 0.5250 - val_loss: 0.8122\n",
      "Epoch 413/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6888 - loss: 0.5668 - val_accuracy: 0.5200 - val_loss: 0.8167\n",
      "Epoch 414/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7077 - loss: 0.5591 - val_accuracy: 0.5300 - val_loss: 0.8107\n",
      "Epoch 415/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6960 - loss: 0.5722 - val_accuracy: 0.5250 - val_loss: 0.8144\n",
      "Epoch 416/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7026 - loss: 0.5660 - val_accuracy: 0.5400 - val_loss: 0.8066\n",
      "Epoch 417/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7213 - loss: 0.5580 - val_accuracy: 0.5550 - val_loss: 0.8007\n",
      "Epoch 418/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7085 - loss: 0.5703 - val_accuracy: 0.5200 - val_loss: 0.8067\n",
      "Epoch 419/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6996 - loss: 0.5712 - val_accuracy: 0.5400 - val_loss: 0.8143\n",
      "Epoch 420/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7173 - loss: 0.5512 - val_accuracy: 0.5450 - val_loss: 0.8114\n",
      "Epoch 421/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6902 - loss: 0.5662 - val_accuracy: 0.5600 - val_loss: 0.8054\n",
      "Epoch 422/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7117 - loss: 0.5556 - val_accuracy: 0.5400 - val_loss: 0.8101\n",
      "Epoch 423/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7254 - loss: 0.5683 - val_accuracy: 0.5350 - val_loss: 0.8134\n",
      "Epoch 424/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6888 - loss: 0.5783 - val_accuracy: 0.5350 - val_loss: 0.8128\n",
      "Epoch 425/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7086 - loss: 0.5830 - val_accuracy: 0.5450 - val_loss: 0.8108\n",
      "Epoch 426/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6872 - loss: 0.5723 - val_accuracy: 0.5350 - val_loss: 0.8186\n",
      "Epoch 427/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6886 - loss: 0.5739 - val_accuracy: 0.5450 - val_loss: 0.8117\n",
      "Epoch 428/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7037 - loss: 0.5721 - val_accuracy: 0.5200 - val_loss: 0.8207\n",
      "Epoch 429/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7029 - loss: 0.5698 - val_accuracy: 0.5250 - val_loss: 0.8203\n",
      "Epoch 430/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7034 - loss: 0.5596 - val_accuracy: 0.5350 - val_loss: 0.8207\n",
      "Epoch 431/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6838 - loss: 0.5746 - val_accuracy: 0.5300 - val_loss: 0.8163\n",
      "Epoch 432/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7026 - loss: 0.5612 - val_accuracy: 0.5400 - val_loss: 0.8195\n",
      "Epoch 433/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6978 - loss: 0.5708 - val_accuracy: 0.5300 - val_loss: 0.8169\n",
      "Epoch 434/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7084 - loss: 0.5651 - val_accuracy: 0.5500 - val_loss: 0.8166\n",
      "Epoch 435/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7345 - loss: 0.5385 - val_accuracy: 0.5300 - val_loss: 0.8210\n",
      "Epoch 436/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6993 - loss: 0.5875 - val_accuracy: 0.5150 - val_loss: 0.8189\n",
      "Epoch 437/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7097 - loss: 0.5650 - val_accuracy: 0.5250 - val_loss: 0.8115\n",
      "Epoch 438/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7080 - loss: 0.5666 - val_accuracy: 0.5500 - val_loss: 0.8156\n",
      "Epoch 439/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7125 - loss: 0.5535 - val_accuracy: 0.5300 - val_loss: 0.8159\n",
      "Epoch 440/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7309 - loss: 0.5521 - val_accuracy: 0.5500 - val_loss: 0.8102\n",
      "Epoch 441/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7403 - loss: 0.5419 - val_accuracy: 0.5200 - val_loss: 0.8188\n",
      "Epoch 442/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7031 - loss: 0.5656 - val_accuracy: 0.5400 - val_loss: 0.8183\n",
      "Epoch 443/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7108 - loss: 0.5551 - val_accuracy: 0.5400 - val_loss: 0.8204\n",
      "Epoch 444/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6856 - loss: 0.5734 - val_accuracy: 0.5300 - val_loss: 0.8201\n",
      "Epoch 445/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6932 - loss: 0.5638 - val_accuracy: 0.5350 - val_loss: 0.8230\n",
      "Epoch 446/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7148 - loss: 0.5600 - val_accuracy: 0.5450 - val_loss: 0.8207\n",
      "Epoch 447/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7266 - loss: 0.5661 - val_accuracy: 0.5300 - val_loss: 0.8199\n",
      "Epoch 448/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7163 - loss: 0.5720 - val_accuracy: 0.5250 - val_loss: 0.8245\n",
      "Epoch 449/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7165 - loss: 0.5632 - val_accuracy: 0.5050 - val_loss: 0.8299\n",
      "Epoch 450/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7011 - loss: 0.5641 - val_accuracy: 0.5200 - val_loss: 0.8279\n",
      "Epoch 451/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7282 - loss: 0.5622 - val_accuracy: 0.5300 - val_loss: 0.8209\n",
      "Epoch 452/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7100 - loss: 0.5748 - val_accuracy: 0.5200 - val_loss: 0.8214\n",
      "Epoch 453/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7298 - loss: 0.5544 - val_accuracy: 0.5300 - val_loss: 0.8308\n",
      "Epoch 454/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6959 - loss: 0.5731 - val_accuracy: 0.5250 - val_loss: 0.8284\n",
      "Epoch 455/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7287 - loss: 0.5546 - val_accuracy: 0.5250 - val_loss: 0.8265\n",
      "Epoch 456/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7234 - loss: 0.5519 - val_accuracy: 0.5300 - val_loss: 0.8343\n",
      "Epoch 457/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7303 - loss: 0.5542 - val_accuracy: 0.5400 - val_loss: 0.8301\n",
      "Epoch 458/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7123 - loss: 0.5676 - val_accuracy: 0.5350 - val_loss: 0.8259\n",
      "Epoch 459/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7210 - loss: 0.5566 - val_accuracy: 0.5150 - val_loss: 0.8365\n",
      "Epoch 460/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7167 - loss: 0.5683 - val_accuracy: 0.5200 - val_loss: 0.8322\n",
      "Epoch 461/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7128 - loss: 0.5543 - val_accuracy: 0.5500 - val_loss: 0.8257\n",
      "Epoch 462/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7008 - loss: 0.5655 - val_accuracy: 0.5450 - val_loss: 0.8190\n",
      "Epoch 463/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7044 - loss: 0.5728 - val_accuracy: 0.5100 - val_loss: 0.8238\n",
      "Epoch 464/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7123 - loss: 0.5487 - val_accuracy: 0.5150 - val_loss: 0.8296\n",
      "Epoch 465/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7269 - loss: 0.5541 - val_accuracy: 0.5200 - val_loss: 0.8348\n",
      "Epoch 466/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7317 - loss: 0.5488 - val_accuracy: 0.5300 - val_loss: 0.8274\n",
      "Epoch 467/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6814 - loss: 0.5720 - val_accuracy: 0.5250 - val_loss: 0.8283\n",
      "Epoch 468/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7115 - loss: 0.5541 - val_accuracy: 0.5250 - val_loss: 0.8305\n",
      "Epoch 469/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7159 - loss: 0.5681 - val_accuracy: 0.5250 - val_loss: 0.8324\n",
      "Epoch 470/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6876 - loss: 0.5678 - val_accuracy: 0.5450 - val_loss: 0.8295\n",
      "Epoch 471/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7322 - loss: 0.5596 - val_accuracy: 0.5200 - val_loss: 0.8415\n",
      "Epoch 472/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7116 - loss: 0.5614 - val_accuracy: 0.5300 - val_loss: 0.8352\n",
      "Epoch 473/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7378 - loss: 0.5535 - val_accuracy: 0.5250 - val_loss: 0.8393\n",
      "Epoch 474/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7177 - loss: 0.5502 - val_accuracy: 0.5450 - val_loss: 0.8354\n",
      "Epoch 475/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7190 - loss: 0.5422 - val_accuracy: 0.5500 - val_loss: 0.8365\n",
      "Epoch 476/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7107 - loss: 0.5576 - val_accuracy: 0.5350 - val_loss: 0.8360\n",
      "Epoch 477/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7136 - loss: 0.5463 - val_accuracy: 0.5500 - val_loss: 0.8319\n",
      "Epoch 478/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7134 - loss: 0.5429 - val_accuracy: 0.5300 - val_loss: 0.8395\n",
      "Epoch 479/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6781 - loss: 0.5730 - val_accuracy: 0.5200 - val_loss: 0.8481\n",
      "Epoch 480/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6990 - loss: 0.5717 - val_accuracy: 0.5250 - val_loss: 0.8493\n",
      "Epoch 481/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7271 - loss: 0.5631 - val_accuracy: 0.5050 - val_loss: 0.8472\n",
      "Epoch 482/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7323 - loss: 0.5300 - val_accuracy: 0.5250 - val_loss: 0.8416\n",
      "Epoch 483/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7428 - loss: 0.5321 - val_accuracy: 0.5250 - val_loss: 0.8421\n",
      "Epoch 484/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7226 - loss: 0.5338 - val_accuracy: 0.5400 - val_loss: 0.8432\n",
      "Epoch 485/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7244 - loss: 0.5583 - val_accuracy: 0.5350 - val_loss: 0.8387\n",
      "Epoch 486/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7160 - loss: 0.5414 - val_accuracy: 0.5350 - val_loss: 0.8400\n",
      "Epoch 487/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7189 - loss: 0.5325 - val_accuracy: 0.5350 - val_loss: 0.8397\n",
      "Epoch 488/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6797 - loss: 0.5717 - val_accuracy: 0.5450 - val_loss: 0.8355\n",
      "Epoch 489/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7164 - loss: 0.5541 - val_accuracy: 0.5200 - val_loss: 0.8407\n",
      "Epoch 490/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7181 - loss: 0.5608 - val_accuracy: 0.5150 - val_loss: 0.8415\n",
      "Epoch 491/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5486 - val_accuracy: 0.5250 - val_loss: 0.8401\n",
      "Epoch 492/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7187 - loss: 0.5539 - val_accuracy: 0.5350 - val_loss: 0.8377\n",
      "Epoch 493/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7110 - loss: 0.5612 - val_accuracy: 0.5200 - val_loss: 0.8411\n",
      "Epoch 494/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7479 - loss: 0.5362 - val_accuracy: 0.5300 - val_loss: 0.8421\n",
      "Epoch 495/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7117 - loss: 0.5564 - val_accuracy: 0.5150 - val_loss: 0.8342\n",
      "Epoch 496/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7093 - loss: 0.5616 - val_accuracy: 0.5150 - val_loss: 0.8390\n",
      "Epoch 497/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7336 - loss: 0.5504 - val_accuracy: 0.5350 - val_loss: 0.8378\n",
      "Epoch 498/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7193 - loss: 0.5432 - val_accuracy: 0.5450 - val_loss: 0.8384\n",
      "Epoch 499/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7011 - loss: 0.5700 - val_accuracy: 0.5450 - val_loss: 0.8350\n",
      "Epoch 500/500\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7095 - loss: 0.5434 - val_accuracy: 0.5250 - val_loss: 0.8397\n",
      "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x13bd8e340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step\n",
      "Predicted Sentiment Score: 0.67339057\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding, Dense, Layer, GlobalAveragePooling1D, LayerNormalization, Add, Dropout\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "VOCAB_SIZE = 20   # Number of unique tokens\n",
    "EMBED_DIM = 8     # Increased from 4 to 8\n",
    "SEQ_LENGTH = 5    # Increased from 3 to 5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "DROPOUT_RATE = 0.1  # Regularization\n",
    "\n",
    "# Dummy dataset (random sequences and binary labels)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randint(0, VOCAB_SIZE, size=(1000, SEQ_LENGTH))\n",
    "y_train = np.random.randint(0, 2, size=(1000, 1))\n",
    "\n",
    "X_val = np.random.randint(0, VOCAB_SIZE, size=(200, SEQ_LENGTH))\n",
    "y_val = np.random.randint(0, 2, size=(200, 1))\n",
    "\n",
    "\n",
    "# Define Improved Self-Attention Layer with Residual Connection & Layer Norm\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.W_q = Dense(embed_dim, use_bias=False)\n",
    "        self.W_k = Dense(embed_dim, use_bias=False)\n",
    "        self.W_v = Dense(embed_dim, use_bias=False)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.add = Add()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Q = self.W_q(inputs)\n",
    "        K = self.W_k(inputs)\n",
    "        V = self.W_v(inputs)\n",
    "\n",
    "        # Compute scaled dot-product attention\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        attention_output = tf.matmul(attention_weights, V)\n",
    "\n",
    "        # Residual connection: Add input embeddings to attention output\n",
    "        output = self.add([attention_output, inputs])\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        return self.layer_norm(output)\n",
    "\n",
    "\n",
    "# Define Model\n",
    "inputs = keras.Input(shape=(SEQ_LENGTH,))\n",
    "\n",
    "# Trainable Embedding Layer (Joint Training)\n",
    "embedding = Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_DIM, trainable=True)(inputs)\n",
    "\n",
    "# Self-Attention Layer with residual connection and LayerNorm\n",
    "attention_output = SelfAttention(EMBED_DIM)(embedding)\n",
    "\n",
    "# Global Average Pooling\n",
    "pooled_output = GlobalAveragePooling1D()(attention_output)\n",
    "\n",
    "# Fully Connected MLP Head\n",
    "fc_output = Dense(16, activation='relu')(pooled_output)  # Increased MLP complexity\n",
    "fc_output = Dropout(DROPOUT_RATE)(fc_output)  # Add dropout for regularization\n",
    "fc_output = Dense(1, activation='sigmoid')(fc_output)\n",
    "\n",
    "# Compile Model\n",
    "model = keras.Model(inputs=inputs, outputs=fc_output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Test Prediction\n",
    "test_sentence = np.array([[3, 5, 7, 9, 1]])  # Example sequence of token indices\n",
    "predicted_sentiment = model.predict(test_sentence)\n",
    "print(\"Predicted Sentiment Score:\", predicted_sentiment[0, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aboves are implementaions with Keras\n",
    "and here comes the numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6932\n",
      "Epoch 10, Loss: 0.6932\n",
      "Epoch 20, Loss: 0.6931\n",
      "Epoch 30, Loss: 0.6931\n",
      "Epoch 40, Loss: 0.6931\n",
      "Predicted Sentiment Score: 0.4984710036731359\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "VOCAB_SIZE = 20   # Number of unique words\n",
    "EMBED_DIM = 4     # Each word is a 4D vector\n",
    "SEQ_LENGTH = 3    # Short sequence length\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Dummy dataset (random token sequences and binary labels)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randint(0, VOCAB_SIZE, size=(1000, SEQ_LENGTH))\n",
    "y_train = np.random.randint(0, 2, size=(1000, 1))\n",
    "\n",
    "X_val = np.random.randint(0, VOCAB_SIZE, size=(200, SEQ_LENGTH))\n",
    "y_val = np.random.randint(0, 2, size=(200, 1))\n",
    "\n",
    "# Randomly initialize word embeddings (trainable)\n",
    "embedding_matrix = np.random.randn(VOCAB_SIZE, EMBED_DIM) * 0.1\n",
    "\n",
    "# Initialize self-attention weight matrices\n",
    "W_q = np.random.randn(EMBED_DIM, EMBED_DIM) * 0.1  # Query\n",
    "W_k = np.random.randn(EMBED_DIM, EMBED_DIM) * 0.1  # Key\n",
    "W_v = np.random.randn(EMBED_DIM, EMBED_DIM) * 0.1  # Value\n",
    "\n",
    "# Initialize MLP layer (final classifier)\n",
    "W_fc = np.random.randn(EMBED_DIM, 1) * 0.1\n",
    "b_fc = np.zeros((1,))\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]  # Embedding dimension\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Scaled dot-product\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)  # Softmax\n",
    "    return np.dot(attention_weights, V)  # Weighted sum of values\n",
    "\n",
    "def self_attention(X):\n",
    "    \"\"\"\n",
    "    Compute self-attention over input embeddings.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embed_dim = X.shape\n",
    "    outputs = np.zeros((batch_size, seq_len, embed_dim))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        Q = np.dot(X[i], W_q)\n",
    "        K = np.dot(X[i], W_k)\n",
    "        V = np.dot(X[i], W_v)\n",
    "        outputs[i] = scaled_dot_product_attention(Q, K, V)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def fully_connected(X):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer after self-attention.\n",
    "    \"\"\"\n",
    "    return np.dot(X.mean(axis=1), W_fc) + b_fc  # Mean pooling over sequence\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    # Forward pass\n",
    "    X_embedded = embedding_matrix[X_train]  # Convert tokens to embeddings\n",
    "    attention_out = self_attention(X_embedded)  # Apply self-attention\n",
    "    logits = fully_connected(attention_out)  # MLP classification\n",
    "    predictions = sigmoid(logits)  # Sigmoid activation\n",
    "\n",
    "    # Compute loss\n",
    "    loss = binary_cross_entropy(y_train, predictions)\n",
    "\n",
    "    # Compute gradients (Simple Gradient Descent)\n",
    "    dW_fc = np.dot(attention_out.mean(axis=1).T, (predictions - y_train)) / len(y_train)\n",
    "    db_fc = np.mean(predictions - y_train)\n",
    "\n",
    "    # Update weights\n",
    "    global W_fc, b_fc\n",
    "    W_fc -= LEARNING_RATE * dW_fc\n",
    "    b_fc -= LEARNING_RATE * db_fc\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test the model with a sample input\n",
    "test_sentence = np.array([[3, 5, 7]])  # Example token sequence\n",
    "test_embedded = embedding_matrix[test_sentence]\n",
    "test_attention = self_attention(test_embedded)\n",
    "test_logits = fully_connected(test_attention)\n",
    "test_prediction = sigmoid(test_logits)\n",
    "\n",
    "print(\"Predicted Sentiment Score:\", test_prediction[0, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Backpropagation through self-attention layer\u001b[39;00m\n\u001b[1;32m     73\u001b[0m d_attention_output \u001b[38;5;241m=\u001b[39m (predictions \u001b[38;5;241m-\u001b[39m y_train)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m W_fc\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m---> 74\u001b[0m dW_q, dW_k, dW_v \u001b[38;5;241m=\u001b[39m \u001b[43mbackpropagate_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_attention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Update MLP weights\u001b[39;00m\n\u001b[1;32m     77\u001b[0m W_fc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m LEARNING_RATE \u001b[38;5;241m*\u001b[39m dW_fc\n",
      "Cell \u001b[0;32mIn[17], line 46\u001b[0m, in \u001b[0;36mbackpropagate_attention\u001b[0;34m(X_embedded, attention_output, d_attention_output)\u001b[0m\n\u001b[1;32m     43\u001b[0m attention_weights \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(attention_weights, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Gradients for Q, K, V\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m dW_q, dW_k, dW_v \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_attention_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_attention_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m dW_q_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dW_q\n\u001b[1;32m     49\u001b[0m dW_k_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dW_k\n",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m, in \u001b[0;36mcompute_attention_gradients\u001b[0;34m(Q, K, V, attention_weights, d_attention_output)\u001b[0m\n\u001b[1;32m     10\u001b[0m d_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(d_attention_weights, V) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(Q\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# scale it with the embedding dimension\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Corrected Gradients of Q, K, V\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m dQ \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fixed\u001b[39;00m\n\u001b[1;32m     14\u001b[0m dK \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Q\u001b[38;5;241m.\u001b[39mT, d_scores)  \u001b[38;5;66;03m# Fixed\u001b[39;00m\n\u001b[1;32m     15\u001b[0m dV \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(attention_weights\u001b[38;5;241m.\u001b[39mT, d_attention_output)  \u001b[38;5;66;03m# Fixed\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,4) and (3,4) not aligned: 4 (dim 1) != 3 (dim 0)"
     ]
    }
   ],
   "source": [
    "def compute_attention_gradients(Q, K, V, attention_weights, d_attention_output):\n",
    "    \"\"\"\n",
    "    Compute the gradients for Query, Key, and Value matrices from the attention output.\n",
    "    \"\"\"\n",
    "    # Gradient of Attention Weights\n",
    "    d_attention_weights = np.dot(d_attention_output, V.T)  # (seq_len, seq_len)\n",
    "    d_attention_weights = attention_weights * (1 - attention_weights) * d_attention_weights\n",
    "    \n",
    "    # Gradient of the scaled dot product scores\n",
    "    d_scores = np.dot(d_attention_weights, V) / np.sqrt(Q.shape[1])  # scale it with the embedding dimension\n",
    "\n",
    "    # Corrected Gradients of Q, K, V\n",
    "    dQ = np.dot(d_scores, K)  # Fixed\n",
    "    dK = np.dot(Q.T, d_scores)  # Fixed\n",
    "    dV = np.dot(attention_weights.T, d_attention_output)  # Fixed\n",
    "\n",
    "    # Compute gradients for W_q, W_k, W_v\n",
    "    dW_q = np.dot(Q.T, dQ)\n",
    "    dW_k = np.dot(K.T, dK)\n",
    "    dW_v = np.dot(V.T, dV)\n",
    "\n",
    "    return dW_q, dW_k, dW_v\n",
    "\n",
    "\n",
    "def backpropagate_attention(X_embedded, attention_output, d_attention_output):\n",
    "    \"\"\"\n",
    "    Perform backpropagation through self-attention and update the weight matrices (W_q, W_k, W_v).\n",
    "    \"\"\"\n",
    "    # Compute the attention gradients\n",
    "    batch_size = X_embedded.shape[0]\n",
    "    dW_q_total = np.zeros_like(W_q)\n",
    "    dW_k_total = np.zeros_like(W_k)\n",
    "    dW_v_total = np.zeros_like(W_v)\n",
    "\n",
    "    # Backpropagate through each example in the batch\n",
    "    for i in range(batch_size):\n",
    "        Q = np.dot(X_embedded[i], W_q)\n",
    "        K = np.dot(X_embedded[i], W_k)\n",
    "        V = np.dot(X_embedded[i], W_v)\n",
    "\n",
    "        # Calculate the gradients for this instance\n",
    "        attention_weights = np.exp(np.dot(Q, K.T) / np.sqrt(Q.shape[1]))\n",
    "        attention_weights /= np.sum(attention_weights, axis=-1, keepdims=True)\n",
    "\n",
    "        # Gradients for Q, K, V\n",
    "        dW_q, dW_k, dW_v = compute_attention_gradients(Q, K, V, attention_weights, d_attention_output[i])\n",
    "\n",
    "        dW_q_total += dW_q\n",
    "        dW_k_total += dW_k\n",
    "        dW_v_total += dW_v\n",
    "\n",
    "    # Average over the batch\n",
    "    dW_q_total /= batch_size\n",
    "    dW_k_total /= batch_size\n",
    "    dW_v_total /= batch_size\n",
    "\n",
    "    return dW_q_total, dW_k_total, dW_v_total\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    X_embedded = embedding_matrix[X_train]  # Convert tokens to embeddings\n",
    "    attention_out = self_attention(X_embedded)  # Apply self-attention\n",
    "    logits = fully_connected(attention_out)  # MLP classification\n",
    "    predictions = sigmoid(logits)  # Sigmoid activation\n",
    "\n",
    "    # Compute loss\n",
    "    loss = binary_cross_entropy(y_train, predictions)\n",
    "\n",
    "    # Compute gradients for MLP weights\n",
    "    dW_fc = np.dot(attention_out.mean(axis=1).T, (predictions - y_train)) / len(y_train)\n",
    "    db_fc = np.mean(predictions - y_train)\n",
    "\n",
    "    # Backpropagation through self-attention layer\n",
    "    d_attention_output = (predictions - y_train).reshape(-1, 1) * W_fc.T\n",
    "    dW_q, dW_k, dW_v = backpropagate_attention(X_embedded, attention_out, d_attention_output)\n",
    "\n",
    "    # Update MLP weights\n",
    "    W_fc -= LEARNING_RATE * dW_fc\n",
    "    b_fc -= LEARNING_RATE * db_fc\n",
    "\n",
    "    # Update attention weights (W_q, W_k, W_v)\n",
    "    W_q -= LEARNING_RATE * dW_q\n",
    "    W_k -= LEARNING_RATE * dW_k\n",
    "    W_v -= LEARNING_RATE * dW_v\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,3) and (4,) not aligned: 3 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 139\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Backpropagation through self-attention layer\u001b[39;00m\n\u001b[1;32m    138\u001b[0m d_attention_output \u001b[38;5;241m=\u001b[39m (predictions \u001b[38;5;241m-\u001b[39m y_train) \u001b[38;5;241m@\u001b[39m W_fc\u001b[38;5;241m.\u001b[39mT  \u001b[38;5;66;03m# Reshape properly\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m dW_q, dW_k, dW_v \u001b[38;5;241m=\u001b[39m \u001b[43mbackpropagate_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_embedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_attention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# Update MLP weights\u001b[39;00m\n\u001b[1;32m    142\u001b[0m W_fc \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m LEARNING_RATE \u001b[38;5;241m*\u001b[39m dW_fc\n",
      "Cell \u001b[0;32mIn[20], line 108\u001b[0m, in \u001b[0;36mbackpropagate_attention\u001b[0;34m(X_embedded, attention_output, attention_weights, d_attention_output)\u001b[0m\n\u001b[1;32m    105\u001b[0m V \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(X_embedded[i], W_v)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Compute gradients for this sample\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m dW_q, dW_k, dW_v \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_attention_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_attention_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Accumulate gradients over batch\u001b[39;00m\n\u001b[1;32m    111\u001b[0m dW_q_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m dW_q\n",
      "Cell \u001b[0;32mIn[20], line 84\u001b[0m, in \u001b[0;36mcompute_attention_gradients\u001b[0;34m(Q, K, V, attention_weights, d_attention_output)\u001b[0m\n\u001b[1;32m     82\u001b[0m dQ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(d_scores, K\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m     83\u001b[0m dK \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Q\u001b[38;5;241m.\u001b[39mT, d_scores)\n\u001b[0;32m---> 84\u001b[0m dV \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_attention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Compute gradients for W_q, W_k, W_v\u001b[39;00m\n\u001b[1;32m     87\u001b[0m dW_q \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(Q\u001b[38;5;241m.\u001b[39mT, dQ)\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,3) and (4,) not aligned: 3 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters\n",
    "VOCAB_SIZE = 20   # Number of unique words\n",
    "EMBED_DIM = 4     # Word vector dimension\n",
    "SEQ_LENGTH = 3    # Sequence length\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Dummy dataset (random token sequences and binary labels)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randint(0, VOCAB_SIZE, size=(1000, SEQ_LENGTH))\n",
    "y_train = np.random.randint(0, 2, size=(1000, 1))\n",
    "\n",
    "X_val = np.random.randint(0, VOCAB_SIZE, size=(200, SEQ_LENGTH))\n",
    "y_val = np.random.randint(0, 2, size=(200, 1))\n",
    "\n",
    "# Randomly initialize trainable embeddings\n",
    "embedding_matrix = np.random.randn(VOCAB_SIZE, EMBED_DIM) * 0.1\n",
    "\n",
    "# Initialize self-attention weight matrices\n",
    "W_q = np.random.randn(EMBED_DIM, EMBED_DIM) * 0.1  # Query\n",
    "W_k = np.random.randn(EMBED_DIM, EMBED_DIM) * 0.1  # Key\n",
    "W_v = np.random.randn(EMBED_DIM, EMBED_DIM) * 0.1  # Value\n",
    "\n",
    "# Initialize MLP layer (fully connected layer)\n",
    "W_fc = np.random.randn(EMBED_DIM, 1) * 0.1\n",
    "b_fc = np.zeros((1,))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred + 1e-8) + (1 - y_true) * np.log(1 - y_pred + 1e-8))\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]  # Embedding dimension\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)  # Scaled dot-product\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)  # Softmax\n",
    "    return np.dot(attention_weights, V), attention_weights  # Weighted sum of values\n",
    "\n",
    "def self_attention(X):\n",
    "    \"\"\"\n",
    "    Compute self-attention over input embeddings.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, embed_dim = X.shape\n",
    "    outputs = np.zeros((batch_size, seq_len, embed_dim))\n",
    "    attention_weights = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        Q = np.dot(X[i].reshape(-1, EMBED_DIM), W_q.T)\n",
    "        K = np.dot(X[i].reshape(-1, EMBED_DIM), W_k.T)\n",
    "        V = np.dot(X[i].reshape(-1, EMBED_DIM), W_v.T)\n",
    "        attention_out, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "        outputs[i] = attention_out\n",
    "        attention_weights.append(attn_weights)\n",
    "\n",
    "    return outputs, np.array(attention_weights)\n",
    "\n",
    "def fully_connected(X):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer after self-attention.\n",
    "    \"\"\"\n",
    "    return np.dot(X.mean(axis=1), W_fc) + b_fc  # Mean pooling over sequence\n",
    "\n",
    "def compute_attention_gradients(Q, K, V, attention_weights, d_attention_output):\n",
    "    \"\"\"\n",
    "    Compute the gradients for Query, Key, and Value matrices from the attention output.\n",
    "    \"\"\"\n",
    "    # Gradient of Attention Weights\n",
    "    d_attention_weights = np.dot(d_attention_output, V.T)\n",
    "    d_attention_weights = attention_weights * (1 - attention_weights) * d_attention_weights\n",
    "    \n",
    "    # Gradient of the scaled dot-product scores\n",
    "    d_scores = np.dot(d_attention_weights, V) / np.sqrt(Q.shape[1])  \n",
    "\n",
    "    # Compute gradients of Q, K, V\n",
    "    dQ = np.dot(d_scores, K.T)\n",
    "    dK = np.dot(Q.T, d_scores)\n",
    "    dV = np.dot(attention_weights.T, d_attention_output)\n",
    "\n",
    "    # Compute gradients for W_q, W_k, W_v\n",
    "    dW_q = np.dot(Q.T, dQ)\n",
    "    dW_k = np.dot(K.T, dK)\n",
    "    dW_v = np.dot(V.T, dV)\n",
    "\n",
    "    return dW_q, dW_k, dW_v\n",
    "\n",
    "def backpropagate_attention(X_embedded, attention_output, attention_weights, d_attention_output):\n",
    "    \"\"\"\n",
    "    Perform backpropagation through self-attention and update W_q, W_k, W_v.\n",
    "    \"\"\"\n",
    "    batch_size = X_embedded.shape[0]\n",
    "    dW_q_total = np.zeros_like(W_q)\n",
    "    dW_k_total = np.zeros_like(W_k)\n",
    "    dW_v_total = np.zeros_like(W_v)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        Q = np.dot(X_embedded[i], W_q)\n",
    "        K = np.dot(X_embedded[i], W_k)\n",
    "        V = np.dot(X_embedded[i], W_v)\n",
    "\n",
    "        # Compute gradients for this sample\n",
    "        dW_q, dW_k, dW_v = compute_attention_gradients(Q, K, V, attention_weights[i], d_attention_output[i])\n",
    "\n",
    "        # Accumulate gradients over batch\n",
    "        dW_q_total += dW_q\n",
    "        dW_k_total += dW_k\n",
    "        dW_v_total += dW_v\n",
    "\n",
    "    # Average over the batch\n",
    "    dW_q_total /= batch_size\n",
    "    dW_k_total /= batch_size\n",
    "    dW_v_total /= batch_size\n",
    "\n",
    "    return dW_q_total, dW_k_total, dW_v_total\n",
    "\n",
    "# Training loop with backpropagation for self-attention weights\n",
    "for epoch in range(EPOCHS):\n",
    "    # Forward pass\n",
    "    X_embedded = embedding_matrix[X_train]  # Convert tokens to embeddings\n",
    "    attention_out, attention_weights = self_attention(X_embedded)  # Apply self-attention\n",
    "    logits = fully_connected(attention_out)  # MLP classification\n",
    "    predictions = sigmoid(logits)  # Sigmoid activation\n",
    "\n",
    "    # Compute loss\n",
    "    loss = binary_cross_entropy(y_train, predictions)\n",
    "\n",
    "    # Compute gradients for MLP weights\n",
    "    dW_fc = np.dot(attention_out.mean(axis=1).T, (predictions - y_train)) / len(y_train)\n",
    "    db_fc = np.mean(predictions - y_train)\n",
    "\n",
    "    # Backpropagation through self-attention layer\n",
    "    d_attention_output = (predictions - y_train) @ W_fc.T  # Reshape properly\n",
    "    dW_q, dW_k, dW_v = backpropagate_attention(X_embedded, attention_out, attention_weights, d_attention_output)\n",
    "\n",
    "    # Update MLP weights\n",
    "    W_fc -= LEARNING_RATE * dW_fc\n",
    "    b_fc -= LEARNING_RATE * db_fc\n",
    "\n",
    "    # Update attention weights (W_q, W_k, W_v)\n",
    "    W_q -= LEARNING_RATE * dW_q\n",
    "    W_k -= LEARNING_RATE * dW_k\n",
    "    W_v -= LEARNING_RATE * dW_v\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test the model\n",
    "test_sentence = np.array([[3, 5, 7]])  # Example token sequence\n",
    "test_embedded = embedding_matrix[test_sentence]\n",
    "test_attention, _ = self_attention(test_embedded)\n",
    "test_logits = fully_connected(test_attention)\n",
    "test_prediction = sigmoid(test_logits)\n",
    "\n",
    "print(\"Predicted Sentiment Score:\", test_prediction[0, 0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mensius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
